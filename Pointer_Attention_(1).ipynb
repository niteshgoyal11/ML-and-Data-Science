{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pointer_Attention (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBHM1KbRCtlC",
        "outputId": "8b5d0aca-a079-44f4-a51c-ca698917e5e3"
      },
      "source": [
        "!pip install texthero\n",
        "!pip install rouge"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting texthero\n",
            "  Downloading texthero-1.1.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: gensim<4.0,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.6.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.0.1)\n",
            "Requirement already satisfied: wordcloud>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.5.0)\n",
            "Requirement already satisfied: plotly>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.4.1)\n",
            "Collecting unidecode>=1.1.1\n",
            "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.62.3)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.2.2)\n",
            "Requirement already satisfied: spacy<3.0.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (2.2.4)\n",
            "Collecting nltk>=3.3\n",
            "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 36.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (3.0.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (1.1.0)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2021.11.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 48.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.2->texthero) (2018.9)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.2.0->texthero) (1.3.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->texthero) (3.0.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (1.24.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud>=1.5.0->texthero) (7.1.2)\n",
            "Installing collected packages: regex, unidecode, nltk, texthero\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.5 regex-2021.11.10 texthero-1.1.0 unidecode-1.3.2\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUPQ8uv9CwAA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a0837ae-208a-470d-ecfe-b3c80d82c718"
      },
      "source": [
        "#importing libraries\n",
        "import texthero as hero\n",
        "from rouge import Rouge\n",
        "from string import digits, punctuation\n",
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import string \n",
        "import os\n",
        "import re\n",
        "import queue\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlO0SbA_C0q7",
        "outputId": "dc35a588-2eb6-4179-cf97-ec1a9aa85a8e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17bYNfGQC4Bk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "79397c20-0721-4133-df05-e11b6272580f"
      },
      "source": [
        "# reading the dataset\n",
        "df_more = pd.read_csv('/content/drive/MyDrive/LSTM/news_summary_more.csv',)\n",
        "df_more.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
              "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
              "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
              "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
              "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
              "      <td>Speaking about the sexual harassment allegatio...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           headlines                                               text\n",
              "0  upGrad learner switches to career in ML & Al w...  Saurav Kant, an alumnus of upGrad and IIIT-B's...\n",
              "1  Delhi techie wins free food from Swiggy for on...  Kunal Shah's credit card bill payment platform...\n",
              "2  New Zealand end Rohit Sharma-led India's 12-ma...  New Zealand defeated India by 8 wickets in the...\n",
              "3  Aegon life iTerm insurance plan helps customer...  With Aegon Life iTerm Insurance plan, customer...\n",
              "4  Have known Hirani for yrs, what if MeToo claim...  Speaking about the sexual harassment allegatio..."
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_uGU8PxI2e8"
      },
      "source": [
        "# calculating the length of the senetnce\n",
        "df_more['head_len'] = df_more['headlines'].apply(lambda x : len(x.split(' ')))\n",
        "df_more['text_len'] = df_more['text'].apply(lambda x : len(x.split(' ')))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dK1DzdtNJCTt",
        "outputId": "43ef580e-2f19-4c9b-ca2f-fea8f6f76cee"
      },
      "source": [
        "df_more.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>text</th>\n",
              "      <th>head_len</th>\n",
              "      <th>text_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
              "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
              "      <td>13</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
              "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
              "      <td>12</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
              "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
              "      <td>9</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
              "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
              "      <td>9</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
              "      <td>Speaking about the sexual harassment allegatio...</td>\n",
              "      <td>13</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           headlines  ... text_len\n",
              "0  upGrad learner switches to career in ML & Al w...  ...       60\n",
              "1  Delhi techie wins free food from Swiggy for on...  ...       60\n",
              "2  New Zealand end Rohit Sharma-led India's 12-ma...  ...       60\n",
              "3  Aegon life iTerm insurance plan helps customer...  ...       60\n",
              "4  Have known Hirani for yrs, what if MeToo claim...  ...       60\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl8qmfpAJL3W",
        "outputId": "2985d633-6e4c-4cd4-f7cc-0adcf1a1fa27"
      },
      "source": [
        "# maximum length of the headlines and text\n",
        "df_more['head_len'].max(), df_more['text_len'].max()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18, 92)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VcB4Ti4JjOY"
      },
      "source": [
        "# dropping the unnecesary columns\n",
        "df_more.drop(['head_len', 'text_len'], axis=1, inplace=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHVedjiGIyEV"
      },
      "source": [
        "#renaming columns\n",
        "df_more.columns = ['highlights', 'stories']"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr7At0R-JzRY"
      },
      "source": [
        "df = df_more.copy(deep=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJJYWEFefqeA"
      },
      "source": [
        "# to attach the device to tensor\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device to run the code on GPU"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "krau17-hNh5j",
        "outputId": "811706f8-08c0-4a78-b5e1-2b75eddb0940"
      },
      "source": [
        "# convertin to str\n",
        "df['stories'] = df['stories'].astype(str)\n",
        "df['highlights'] = df['highlights'].astype(str)\n",
        "\n",
        "# removing digits\n",
        "remove_digits = str.maketrans('', '', digits)\n",
        "df['stories'] = df['stories'].apply(lambda x : x.translate(remove_digits))\n",
        "df['highlights'] = df['highlights'].apply(lambda x : x.translate(remove_digits))\n",
        "\n",
        "df['stories_len'] = df['stories'].apply(lambda x : len(x.split(' ')))\n",
        "df['highlights_len'] = df['highlights'].apply(lambda x : len(x.split(' ')))\n",
        "df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>highlights</th>\n",
              "      <th>stories</th>\n",
              "      <th>stories_len</th>\n",
              "      <th>highlights_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
              "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
              "      <td>60</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
              "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
              "      <td>60</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Zealand end Rohit Sharma-led India's -matc...</td>\n",
              "      <td>New Zealand defeated India by  wickets in the ...</td>\n",
              "      <td>60</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
              "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
              "      <td>60</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
              "      <td>Speaking about the sexual harassment allegatio...</td>\n",
              "      <td>60</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          highlights  ... highlights_len\n",
              "0  upGrad learner switches to career in ML & Al w...  ...             13\n",
              "1  Delhi techie wins free food from Swiggy for on...  ...             12\n",
              "2  New Zealand end Rohit Sharma-led India's -matc...  ...              9\n",
              "3  Aegon life iTerm insurance plan helps customer...  ...              9\n",
              "4  Have known Hirani for yrs, what if MeToo claim...  ...             13\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz2qCr1hNqcC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16c18172-a64c-4074-f313-47ac5d685c9d"
      },
      "source": [
        "# selecting the samples having max sentence length = 60\n",
        "max_sentence_length = 60\n",
        "df = df[df['stories_len'] <= max_sentence_length]\n",
        "df.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(91208, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfZjhB_1ap9y"
      },
      "source": [
        "# pre-processing the entire dataset\n",
        "df['stories'] = hero.clean(df['stories'])\n",
        "df['highlights'] = hero.clean(df['highlights'])\n",
        "\n",
        "# adding the start and the end token\n",
        "df['stories'] = df['stories'].apply(lambda x : 'sos ' + x + ' eos')\n",
        "df['highlights'] = df['highlights'].apply(lambda x : 'sos ' + x + ' eos')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54gyq-SZa2v3"
      },
      "source": [
        "# reading the numpy values\n",
        "stories_ = df.iloc[:,1].values\n",
        "highlights_ = df.iloc[:,0].values\n",
        "output_len =  df.iloc[:,3].values"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Hn8sLr7LbME",
        "outputId": "f5854116-be1e-4a09-8141-41dfd0b442a2"
      },
      "source": [
        "# downloading the word embedding \n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-09 05:21:38--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-12-09 05:21:38--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-09 05:21:38--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.19MB/s    in 2m 40s  \n",
            "\n",
            "2021-12-09 05:24:18 (5.15 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4u_K_EKMVMG"
      },
      "source": [
        "# reading the word eme\n",
        "embedding_dict = dict()\n",
        "f = open('/content/glove.6B.300d.txt')\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coef = np.asarray(values[1:],dtype='float32')\n",
        "    embedding_dict[word] = coef \n",
        "f.close()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT2IDfc4NNZ-"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8pFmIIxbJay"
      },
      "source": [
        "# create vocab\n",
        "words = [] \n",
        "\n",
        "for line in stories_:\n",
        "    for word in line.split(' '):\n",
        "        words.append(word)\n",
        "\n",
        "for line in highlights_:\n",
        "    for word in line.split(' '):\n",
        "        words.append(word)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6JxsvfJbT9L"
      },
      "source": [
        "# return svocabulary of the datsset\n",
        "vocab = sorted(Counter(words), key=Counter(words).get, reverse=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6mtmKPbbe8h"
      },
      "source": [
        "embed_dim = 300\n",
        "words_found = 0\n",
        "words_OOV = 0\n",
        "word2idx = {}\n",
        "word2idx['<pad>'] = 0\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "    word2idx[word] = index+1\n",
        "\n",
        "weight_embedding = np.zeros([len(vocab)+1,embed_dim],dtype='float32')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgmuWzZzb0MR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e610ed7-dda1-40e1-db53-6e7e123969b1"
      },
      "source": [
        "# creating the weight matrix for traniing and calculating out of vocabulary words\n",
        "for i, word in enumerate(vocab):\n",
        "    try:\n",
        "        weight_embedding[i,:] = embedding_dict[word]\n",
        "        words_found += 1\n",
        "    except:\n",
        "        weight_embedding[i,:] = np.random.normal(scale=0.6, size=[embed_dim,])\n",
        "words_found        "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57554"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amUkQan_fzGf"
      },
      "source": [
        "# converting pre-trained word embedding dictionary to the numpy array\n",
        "weight_embedding = torch.from_numpy(weight_embedding)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaDSmFDqcKsI"
      },
      "source": [
        "# to integers\n",
        "\n",
        "stories_num = []\n",
        "highlights_num = []\n",
        "\n",
        "for w in stories_:\n",
        "    sentence = []\n",
        "    for word in w.split():\n",
        "        sentence.extend([word2idx[word]])\n",
        "    stories_num.append(sentence)\n",
        "\n",
        "for w in highlights_:\n",
        "    sentence = []\n",
        "    for word in w.split():\n",
        "        sentence.extend([word2idx[word]])\n",
        "    highlights_num.append(sentence)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZNnEYCMcVmx"
      },
      "source": [
        "# max length of summary and the text\n",
        "max_story_len = 60 \n",
        "max_summary_len = 18\n",
        "story_feature = np.zeros((len(stories_num),max_story_len),dtype=int)\n",
        "highlight_feature = np.zeros((len(highlights_num),max_summary_len), dtype=int)\n",
        "\n",
        "# creating the numy array of the datset \n",
        "for i, row in enumerate(stories_num):\n",
        "    story_feature[i,:len(row)] = np.array(row)[:max_story_len]\n",
        "for i, row in enumerate(highlights_num):\n",
        "    highlight_feature[i,:len(row)] = np.array(row)[:max_summary_len]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi9tkzBIdpMJ"
      },
      "source": [
        "# train test split\n",
        "split_index = int(len(story_feature)*0.8)\n",
        "train_story, test_story = story_feature[:split_index],story_feature[split_index:]\n",
        "train_highlight, test_highlight = highlight_feature[:split_index],highlight_feature[split_index:]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiiFRVL7eLLp"
      },
      "source": [
        "# splits the dattaset\n",
        "split_index = int(len(test_story)*0.1)\n",
        "val_story, test_story  = test_story[:split_index],test_story[split_index:]\n",
        "val_highlight, test_highlight  = test_highlight[:split_index],test_highlight[split_index:]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5be57gDgGi4",
        "outputId": "72a45b9f-43ae-4738-bc87-89323fbb54f7"
      },
      "source": [
        "val_story.shape, test_story.shape, train_story.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1824, 60), (16418, 60), (72966, 60))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15Yx6CBFgLuo",
        "outputId": "e4db895e-9ef2-4407-f49c-5870d18d68f0"
      },
      "source": [
        "val_highlight.shape, test_highlight.shape, train_highlight.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1824, 18), (16418, 18), (72966, 18))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1d2yEWFgUaA"
      },
      "source": [
        "# initializes the LSTM model embedding layer\n",
        "def create_emb(weight_matrix, non_trainable=False):\n",
        "    emb_layer = torch.nn.Embedding(weight_matrix.shape[0],weight_matrix.shape[1])\n",
        "    emb_layer.load_state_dict({'weight': weight_matrix})\n",
        "    if non_trainable:\n",
        "        emb_layer.weight.requires_grad = False \n",
        "    return emb_layer"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7ik9w3OgbCg"
      },
      "source": [
        "# LSTM model encoder network\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dropout, num_hidden, layers, weight_matrix, embed_dim, device):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.layers = layers\n",
        "        self.num_hidden = num_hidden\n",
        "        self.device = device\n",
        "        self.embed_dim = embed_dim\n",
        "        \n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.embedding = create_emb(weight_matrix,True)\n",
        "        # encoder LSTM\n",
        "        self.lstm = torch.nn.LSTM(input_size = self.embed_dim,hidden_size = self.num_hidden,\n",
        "                                  num_layers = self.layers,batch_first=True,dropout=0.5,\n",
        "                                 bidirectional=True)\n",
        "    def forward(self,x,hidden):\n",
        "        x = self.embedding(x)\n",
        "        lstm_in = self.dropout((x))\n",
        "        output,(h_hidden,c_hidden) = self.lstm(lstm_in,hidden)\n",
        "       \n",
        "        h = torch.cat((h_hidden[-1,:,:],h_hidden[-2,:,:]),1)\n",
        "        c = torch.cat((c_hidden[-1,:,:],c_hidden[-2,:,:]),1)\n",
        "\n",
        "        hidden = (h.unsqueeze(0),c.unsqueeze(0))\n",
        "       \n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self,batch_size):\n",
        "        # initialize the hidden state as the zero tensor\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.layers*2,batch_size, self.num_hidden).zero_(),\n",
        "                  weight.new(self.layers*2, batch_size, self.num_hidden).zero_())\n",
        "        return hidden"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-TwHERzg-9p"
      },
      "source": [
        "# attention decoder network with pointer mechanism\n",
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self,num_hidden,dropout,vocab_size,layers,weight_matrix,embed_dims,device):\n",
        "        super(AttentionDecoder,self).__init__()\n",
        "        self.num_hidden = num_hidden\n",
        "        self.dropout = dropout\n",
        "        self.layers = layers\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dims = embed_dims\n",
        "        self.device = device\n",
        "        self.dropout_layer = torch.nn.Dropout(self.dropout)\n",
        "        \n",
        "        self.V = torch.nn.Linear(self.num_hidden,1)\n",
        "        \n",
        "        self.generator = torch.nn.Linear(2*self.num_hidden+self.embed_dims,1)\n",
        "        self.output_layer = torch.nn.Linear(2*self.num_hidden,self.vocab_size)\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "        self.lstm = torch.nn.LSTM(input_size = self.num_hidden+self.embed_dims,hidden_size = self.num_hidden,\n",
        "                                  num_layers = self.layers,batch_first=True,dropout=0.5,\n",
        "                                 bidirectional=False)\n",
        "        self.embedding = create_emb(weight_matrix,True)\n",
        "        self.sig = torch.nn.Sigmoid()\n",
        "        self.device = device\n",
        "        \n",
        "    # forward computation\n",
        "    def forward(self,x,enc_out,hidden,text,batch_size):\n",
        "        x = self.embedding(x).unsqueeze(1)\n",
        "        x = self.dropout_layer(x)\n",
        "        \n",
        "        dec_a = hidden[0].permute(1,0,2)\n",
        "        enc_score = self.V(torch.tanh(enc_out + dec_a)) # attention score\n",
        "        enc_weight = self.softmax(enc_score) # attention weight\n",
        "        enc_context = torch.mul(enc_weight,enc_out) # find the context vector\n",
        "        enc_context = enc_context.sum(1)\n",
        "        enc_context.unsqueeze_(1)\n",
        "        \n",
        "        d_in = torch.cat((x,enc_context),2)\n",
        "        \n",
        "        d_output, hidden = self.lstm(d_in,hidden)\n",
        "        output = torch.cat((d_output.squeeze(1),enc_context.squeeze(1)),1)\n",
        "        \n",
        "        output_generator = torch.cat((enc_context.squeeze(1),d_output.squeeze(1),x.squeeze(1)),1)\n",
        "        \n",
        "        # pointer mechanism\n",
        "        p_gen = self.sig(self.generator(output_generator).squeeze(1))\n",
        "        \n",
        "        p_pointer = 1 - p_gen\n",
        "        pointer_prob = torch.zeros([batch_size,self.vocab_size],device=self.device)\n",
        "        for i in range(batch_size):\n",
        "            pointer_prob[i,text[i,:]] = enc_weight[i,:,0] # pointer probability weights are the attention scores\n",
        "        generator_prob = self.output_layer(output) # output layer to get vocabulary probability\n",
        "        output_probability = torch.mul(p_pointer.unsqueeze(1),pointer_prob) + torch.mul(p_gen.unsqueeze(1),generator_prob)\n",
        "        \n",
        "        return output_probability, hidden\n",
        "    \n",
        "    def init_hidden(self,batch_size):\n",
        "        # initialize the hidden state as the zero tensor\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.layers, batch_size, 2*self.num_hidden).zero_(),\n",
        "                  weight.new(self.layers, batch_size, 2*self.num_hidden).zero_())\n",
        "        return hidden"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLCJZyiphQ6h"
      },
      "source": [
        "#model hyperparameters\n",
        "dropout = 0.5\n",
        "num_hidden = 256 \n",
        "enc_layers = 1 \n",
        "batch_size = 256\n",
        "vocab_size = len(word2idx) \n",
        "dec_layers = 1 \n",
        "embed_dims = 300  "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt806FkmiBvS"
      },
      "source": [
        "# yiedls batch of data while training the model\n",
        "def get_batches(x, y,batch_size=16):\n",
        "    n_batches = len(x)//batch_size\n",
        "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
        "    for ii in range(0, len(x), batch_size):\n",
        "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moTS0cyyeUlE"
      },
      "source": [
        "class BeamNode(object):\n",
        "    def __init__(self,hidden_state,seq,prob,length,loss):\n",
        "        self.hidden = hidden_state\n",
        "        self.seq = seq\n",
        "        self.prob = prob\n",
        "        self.len = length\n",
        "        self.loss = loss\n",
        "        self.s = self.score()\n",
        "        \n",
        "    def score(self):\n",
        "        return self.prob/float(self.len-1+1e-6) # calculates the score of a sequence\n",
        "\n",
        "# initializes the LSTM attention and pointer model \n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,dropout,num_hidden,enc_layers,weight_embedding,\n",
        "                vocab_size,dec_layers,embed_dims,device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.device = device\n",
        "        self.encoder = Encoder(dropout,num_hidden,enc_layers,weight_embedding,embed_dims,device)\n",
        "        self.decoder = AttentionDecoder(2*num_hidden,dropout,vocab_size,dec_layers,weight_embedding,embed_dims,device)\n",
        "    \n",
        "    def forward(self,x,target,e_hidden,criterion,batch_size):\n",
        "        # training the decoder\n",
        "        loss = 0\n",
        "        prediction = target[:,0].unsqueeze(1) \n",
        "        enc_output,enc_hidden = self.encoder(x,e_hidden) \n",
        "        d_hidden = enc_hidden \n",
        "        dec_input = target[:,0] \n",
        "        for t in range(1,target.shape[1]):\n",
        "            logits, d_hidden = self.decoder(dec_input,enc_output,d_hidden,x,batch_size)\n",
        "            dec_input = target[:,t] # teacher forcing turned on \n",
        "            loss += criterion(logits,target[:,t]) # calculate the loss function\n",
        "            prediction = torch.cat((prediction,torch.argmax(logits,dim=0).unsqueeze(1)),0)\n",
        "        return loss, prediction \n",
        "    \n",
        "    # greedy loss\n",
        "    def inference_greedy(self,x,target,e_hidden,criterion,batch_size):\n",
        "        loss = 0\n",
        "        prediction = target[:,0].unsqueeze(1) \n",
        "        enc_output,enc_hidden = self.encoder(x,e_hidden) \n",
        "        d_hidden = enc_hidden \n",
        "        dec_input = target[:,0] \n",
        "\n",
        "        for t in range(1,target.shape[1]):\n",
        "            # run the decoder\n",
        "            logits, d_hidden = self.decoder(dec_input,enc_output,d_hidden,x,batch_size)\n",
        "            dec_input = torch.argmax(logits,dim=1)\n",
        "            loss += criterion(logits,target[:,t]) # calculate the loss\n",
        "            prediction = torch.cat((prediction,torch.argmax(logits,dim=0).unsqueeze(1)),0)\n",
        "        return loss, prediction\n",
        "\n",
        "    # used to predict the output senetence after training the model\n",
        "    def infer(self,x,target,e_hidden,batch_size):\n",
        "        loss = 0\n",
        "        prediction = target[:,0].unsqueeze(1) \n",
        "        enc_output,enc_hidden = self.encoder(x,e_hidden) \n",
        "        d_hidden = enc_hidden \n",
        "        dec_input = target[:,0] \n",
        "\n",
        "        li = np.zeros((256, 18))\n",
        "        i = 0\n",
        "\n",
        "        for t in range(1,target.shape[1]):\n",
        "            # run the decoder\n",
        "            logits, d_hidden = self.decoder(dec_input,enc_output,d_hidden,x,batch_size)\n",
        "            dec_input = torch.argmax(logits,dim=1)\n",
        "            li[:, i] = torch.argmax(logits, dim=1).cpu()\n",
        "            i += 1\n",
        "        return li\n",
        "    \n",
        "    # for calculating the beam loss\n",
        "    def inference_beam(self,x,target,e_hidden,criterion,beam_width,batch_size):\n",
        "        decoded = [] \n",
        "        losses = 0\n",
        "        enc_output,enc_hidden = self.encoder(x,e_hidden) # run the encoder\n",
        "        for i in range(batch_size):\n",
        "            # for each sentence in the batch\n",
        "            prediction = target[i,0].view([1]).unsqueeze(1) \n",
        "            dec_hidden = enc_hidden[0].permute(1,0,2)[i,:,:].unsqueeze(0)\n",
        "            dec_input = target[i,0].view([1])\n",
        "            d_hidden = (enc_hidden[0][:,i,:].unsqueeze(0),enc_hidden[1][:,i,:].unsqueeze(0))\n",
        "            first_node = BeamNode(d_hidden,dec_input,0,1,0) \n",
        "            nodes = queue.PriorityQueue(maxsize=beam_width) \n",
        "            nodes.put((-first_node.score(),first_node))\n",
        "            for t in range(1,target.shape[1]):\n",
        "                candidatenodes = [] \n",
        "                candidatescore = [] \n",
        "                donenodes = [] \n",
        "\n",
        "                \n",
        "                for _ in range(nodes.qsize()):\n",
        "                    # for each sequence in the queue\n",
        "                    sc,nodex = nodes.get()\n",
        "                    seq = nodex.seq.view([-1,1]) \n",
        "                    dec_input = seq[-1,:] \n",
        "                    hidden = nodex.hidden \n",
        "                    d_hidden = (hidden[0],hidden[1]) \n",
        "\n",
        "                    if dec_input == word2idx['eos']:\n",
        "                        donenodes.append((sc,nodex))\n",
        "                    else:\n",
        "                        # run the decoder\n",
        "                        logits, d_hidden = self.decoder(dec_input,enc_output[i,:,:].unsqueeze(0),d_hidden,x,batch_size=1)\n",
        "                        # calculate the loss\n",
        "                        loss = criterion(logits,target[i,t].view([1]))\n",
        "                        # pick the top 10 logits values\n",
        "                        log_p, index = torch.topk(logits,beam_width)\n",
        "                        for k in range(beam_width):\n",
        "                            node = BeamNode(d_hidden, torch.cat([nodex.seq,index[0,k].unsqueeze(0)]),nodex.prob+log_p[0][k],nodex.len+1,nodex.loss+loss)\n",
        "                            score = -node.score()\n",
        "                            candidatenodes.append([score,node])\n",
        "                for score,n in donenodes:\n",
        "                    nodes.put((score,n))\n",
        "                for score,n in candidatenodes[nodes.qsize():beam_width]:\n",
        "                    nodes.put((score,n))\n",
        "            _,output_node = nodes.get()\n",
        "            \n",
        "            decoded.append(output_node.seq)\n",
        "            losses+=output_node.loss\n",
        "            del nodes\n",
        "        return losses/batch_size,decoded\n",
        "    "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF4fu4eniJ5q",
        "outputId": "6b9aed13-9190-4d4a-9925-3faad0b5c1ba"
      },
      "source": [
        "# hyperparameters of the model\n",
        "epochs = 5\n",
        "batch_size = 256\n",
        "learning_rate = 0.001\n",
        "\n",
        "#initializing the model\n",
        "model = Seq2Seq(dropout,num_hidden,enc_layers,weight_embedding,\n",
        "                vocab_size,dec_layers,embed_dims,device).to(device)\n",
        "\n",
        "#model sscheduler and optimizer object\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "counter = 0\n",
        "loss = []\n",
        "\n",
        "# predicts 3 sentences at each step\n",
        "beam_width = 3 \n",
        "\n",
        "# iterating the model over the given number of epochs\n",
        "for e in range(epochs):\n",
        "    start_time = time.time() # start timer\n",
        "    p = np.random.permutation(train_highlight.shape[0])\n",
        "    train_story = train_story[p,:]\n",
        "    train_highlight = train_highlight[p,:]\n",
        "    e_hidden = model.encoder.init_hidden(batch_size)\n",
        "    \n",
        "    for x,y in get_batches(train_story,train_highlight,batch_size):\n",
        "        model.train()\n",
        "\n",
        "        # attching cuda to the tensor\n",
        "        x = torch.from_numpy(x).to(device)\n",
        "        y = torch.from_numpy(y).to(device)\n",
        "        \n",
        "        e_hidden = tuple([each.data for each in e_hidden])\n",
        "        #zeroing out the gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # model prediction\n",
        "        l,prediction = model(x,y,e_hidden,criterion,batch_size)\n",
        "        loss.append(l.item()) \n",
        "        l.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(),2) \n",
        "        optimizer.step() \n",
        "        \n",
        "        print(\"Epoch: {}/{} \\tStep: {} \\tLoss: {:.4f} \".format(e+1, epochs, counter,l.item()))\n",
        "        counter += 1\n",
        "        \n",
        "\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/LSTM/weight.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning:\n",
            "\n",
            "dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5 \tStep: 0 \tLoss: 123.8170 \n",
            "Epoch: 1/5 \tStep: 1 \tLoss: 123.4164 \n",
            "Epoch: 1/5 \tStep: 2 \tLoss: 133.8129 \n",
            "Epoch: 1/5 \tStep: 3 \tLoss: 121.4609 \n",
            "Epoch: 1/5 \tStep: 4 \tLoss: 116.5439 \n",
            "Epoch: 1/5 \tStep: 5 \tLoss: 123.1975 \n",
            "Epoch: 1/5 \tStep: 6 \tLoss: 104.8104 \n",
            "Epoch: 1/5 \tStep: 7 \tLoss: 92.4947 \n",
            "Epoch: 1/5 \tStep: 8 \tLoss: 96.6749 \n",
            "Epoch: 1/5 \tStep: 9 \tLoss: 93.7161 \n",
            "Epoch: 1/5 \tStep: 10 \tLoss: 94.5865 \n",
            "Epoch: 1/5 \tStep: 11 \tLoss: 98.9296 \n",
            "Epoch: 1/5 \tStep: 12 \tLoss: 98.4534 \n",
            "Epoch: 1/5 \tStep: 13 \tLoss: 96.5698 \n",
            "Epoch: 1/5 \tStep: 14 \tLoss: 97.1911 \n",
            "Epoch: 1/5 \tStep: 15 \tLoss: 95.4838 \n",
            "Epoch: 1/5 \tStep: 16 \tLoss: 97.7337 \n",
            "Epoch: 1/5 \tStep: 17 \tLoss: 92.6184 \n",
            "Epoch: 1/5 \tStep: 18 \tLoss: 95.7796 \n",
            "Epoch: 1/5 \tStep: 19 \tLoss: 97.6994 \n",
            "Epoch: 1/5 \tStep: 20 \tLoss: 95.7214 \n",
            "Epoch: 1/5 \tStep: 21 \tLoss: 96.1197 \n",
            "Epoch: 1/5 \tStep: 22 \tLoss: 96.4624 \n",
            "Epoch: 1/5 \tStep: 23 \tLoss: 100.5777 \n",
            "Epoch: 1/5 \tStep: 24 \tLoss: 94.2840 \n",
            "Epoch: 1/5 \tStep: 25 \tLoss: 94.0927 \n",
            "Epoch: 1/5 \tStep: 26 \tLoss: 92.6575 \n",
            "Epoch: 1/5 \tStep: 27 \tLoss: 91.9938 \n",
            "Epoch: 1/5 \tStep: 28 \tLoss: 97.7172 \n",
            "Epoch: 1/5 \tStep: 29 \tLoss: 91.6552 \n",
            "Epoch: 1/5 \tStep: 30 \tLoss: 97.8581 \n",
            "Epoch: 1/5 \tStep: 31 \tLoss: 91.3139 \n",
            "Epoch: 1/5 \tStep: 32 \tLoss: 91.6184 \n",
            "Epoch: 1/5 \tStep: 33 \tLoss: 88.4071 \n",
            "Epoch: 1/5 \tStep: 34 \tLoss: 92.6367 \n",
            "Epoch: 1/5 \tStep: 35 \tLoss: 92.7202 \n",
            "Epoch: 1/5 \tStep: 36 \tLoss: 87.1615 \n",
            "Epoch: 1/5 \tStep: 37 \tLoss: 90.8709 \n",
            "Epoch: 1/5 \tStep: 38 \tLoss: 90.9580 \n",
            "Epoch: 1/5 \tStep: 39 \tLoss: 88.9805 \n",
            "Epoch: 1/5 \tStep: 40 \tLoss: 85.1184 \n",
            "Epoch: 1/5 \tStep: 41 \tLoss: 89.8164 \n",
            "Epoch: 1/5 \tStep: 42 \tLoss: 91.3562 \n",
            "Epoch: 1/5 \tStep: 43 \tLoss: 84.9604 \n",
            "Epoch: 1/5 \tStep: 44 \tLoss: 90.7065 \n",
            "Epoch: 1/5 \tStep: 45 \tLoss: 83.7913 \n",
            "Epoch: 1/5 \tStep: 46 \tLoss: 88.3941 \n",
            "Epoch: 1/5 \tStep: 47 \tLoss: 89.1973 \n",
            "Epoch: 1/5 \tStep: 48 \tLoss: 84.9919 \n",
            "Epoch: 1/5 \tStep: 49 \tLoss: 86.1195 \n",
            "Epoch: 1/5 \tStep: 50 \tLoss: 89.8689 \n",
            "Epoch: 1/5 \tStep: 51 \tLoss: 85.4773 \n",
            "Epoch: 1/5 \tStep: 52 \tLoss: 89.3414 \n",
            "Epoch: 1/5 \tStep: 53 \tLoss: 84.8570 \n",
            "Epoch: 1/5 \tStep: 54 \tLoss: 88.6719 \n",
            "Epoch: 1/5 \tStep: 55 \tLoss: 91.2385 \n",
            "Epoch: 1/5 \tStep: 56 \tLoss: 89.3890 \n",
            "Epoch: 1/5 \tStep: 57 \tLoss: 89.8062 \n",
            "Epoch: 1/5 \tStep: 58 \tLoss: 86.4193 \n",
            "Epoch: 1/5 \tStep: 59 \tLoss: 93.7128 \n",
            "Epoch: 1/5 \tStep: 60 \tLoss: 85.4712 \n",
            "Epoch: 1/5 \tStep: 61 \tLoss: 85.1014 \n",
            "Epoch: 1/5 \tStep: 62 \tLoss: 84.3525 \n",
            "Epoch: 1/5 \tStep: 63 \tLoss: 84.8109 \n",
            "Epoch: 1/5 \tStep: 64 \tLoss: 89.1072 \n",
            "Epoch: 1/5 \tStep: 65 \tLoss: 84.7554 \n",
            "Epoch: 1/5 \tStep: 66 \tLoss: 88.6141 \n",
            "Epoch: 1/5 \tStep: 67 \tLoss: 82.9389 \n",
            "Epoch: 1/5 \tStep: 68 \tLoss: 88.2675 \n",
            "Epoch: 1/5 \tStep: 69 \tLoss: 86.1038 \n",
            "Epoch: 1/5 \tStep: 70 \tLoss: 86.1212 \n",
            "Epoch: 1/5 \tStep: 71 \tLoss: 86.5099 \n",
            "Epoch: 1/5 \tStep: 72 \tLoss: 83.4713 \n",
            "Epoch: 1/5 \tStep: 73 \tLoss: 85.0615 \n",
            "Epoch: 1/5 \tStep: 74 \tLoss: 86.7704 \n",
            "Epoch: 1/5 \tStep: 75 \tLoss: 83.1250 \n",
            "Epoch: 1/5 \tStep: 76 \tLoss: 87.4947 \n",
            "Epoch: 1/5 \tStep: 77 \tLoss: 95.6612 \n",
            "Epoch: 1/5 \tStep: 78 \tLoss: 83.0182 \n",
            "Epoch: 1/5 \tStep: 79 \tLoss: 85.5900 \n",
            "Epoch: 1/5 \tStep: 80 \tLoss: 97.5593 \n",
            "Epoch: 1/5 \tStep: 81 \tLoss: 87.1092 \n",
            "Epoch: 1/5 \tStep: 82 \tLoss: 82.0376 \n",
            "Epoch: 1/5 \tStep: 83 \tLoss: 83.9627 \n",
            "Epoch: 1/5 \tStep: 84 \tLoss: 87.3034 \n",
            "Epoch: 1/5 \tStep: 85 \tLoss: 87.4099 \n",
            "Epoch: 1/5 \tStep: 86 \tLoss: 87.4709 \n",
            "Epoch: 1/5 \tStep: 87 \tLoss: 87.3112 \n",
            "Epoch: 1/5 \tStep: 88 \tLoss: 81.9010 \n",
            "Epoch: 1/5 \tStep: 89 \tLoss: 84.6029 \n",
            "Epoch: 1/5 \tStep: 90 \tLoss: 86.4669 \n",
            "Epoch: 1/5 \tStep: 91 \tLoss: 82.5146 \n",
            "Epoch: 1/5 \tStep: 92 \tLoss: 86.4824 \n",
            "Epoch: 1/5 \tStep: 93 \tLoss: 82.5630 \n",
            "Epoch: 1/5 \tStep: 94 \tLoss: 81.6382 \n",
            "Epoch: 1/5 \tStep: 95 \tLoss: 81.8765 \n",
            "Epoch: 1/5 \tStep: 96 \tLoss: 83.4432 \n",
            "Epoch: 1/5 \tStep: 97 \tLoss: 92.2014 \n",
            "Epoch: 1/5 \tStep: 98 \tLoss: 82.2181 \n",
            "Epoch: 1/5 \tStep: 99 \tLoss: 85.8564 \n",
            "Epoch: 1/5 \tStep: 100 \tLoss: 93.5342 \n",
            "Epoch: 1/5 \tStep: 101 \tLoss: 91.4263 \n",
            "Epoch: 1/5 \tStep: 102 \tLoss: 84.8381 \n",
            "Epoch: 1/5 \tStep: 103 \tLoss: 82.4539 \n",
            "Epoch: 1/5 \tStep: 104 \tLoss: 85.6089 \n",
            "Epoch: 1/5 \tStep: 105 \tLoss: 86.3607 \n",
            "Epoch: 1/5 \tStep: 106 \tLoss: 87.3635 \n",
            "Epoch: 1/5 \tStep: 107 \tLoss: 80.1388 \n",
            "Epoch: 1/5 \tStep: 108 \tLoss: 85.1489 \n",
            "Epoch: 1/5 \tStep: 109 \tLoss: 81.8719 \n",
            "Epoch: 1/5 \tStep: 110 \tLoss: 85.6648 \n",
            "Epoch: 1/5 \tStep: 111 \tLoss: 83.7506 \n",
            "Epoch: 1/5 \tStep: 112 \tLoss: 84.3001 \n",
            "Epoch: 1/5 \tStep: 113 \tLoss: 80.8368 \n",
            "Epoch: 1/5 \tStep: 114 \tLoss: 84.7152 \n",
            "Epoch: 1/5 \tStep: 115 \tLoss: 86.2426 \n",
            "Epoch: 1/5 \tStep: 116 \tLoss: 86.1083 \n",
            "Epoch: 1/5 \tStep: 117 \tLoss: 86.0712 \n",
            "Epoch: 1/5 \tStep: 118 \tLoss: 84.4071 \n",
            "Epoch: 1/5 \tStep: 119 \tLoss: 81.3935 \n",
            "Epoch: 1/5 \tStep: 120 \tLoss: 85.8837 \n",
            "Epoch: 1/5 \tStep: 121 \tLoss: 80.2126 \n",
            "Epoch: 1/5 \tStep: 122 \tLoss: 85.3136 \n",
            "Epoch: 1/5 \tStep: 123 \tLoss: 84.4908 \n",
            "Epoch: 1/5 \tStep: 124 \tLoss: 86.2244 \n",
            "Epoch: 1/5 \tStep: 125 \tLoss: 84.4461 \n",
            "Epoch: 1/5 \tStep: 126 \tLoss: 83.4425 \n",
            "Epoch: 1/5 \tStep: 127 \tLoss: 84.0960 \n",
            "Epoch: 1/5 \tStep: 128 \tLoss: 83.6149 \n",
            "Epoch: 1/5 \tStep: 129 \tLoss: 81.4061 \n",
            "Epoch: 1/5 \tStep: 130 \tLoss: 86.5349 \n",
            "Epoch: 1/5 \tStep: 131 \tLoss: 92.6639 \n",
            "Epoch: 1/5 \tStep: 132 \tLoss: 84.0099 \n",
            "Epoch: 1/5 \tStep: 133 \tLoss: 81.8188 \n",
            "Epoch: 1/5 \tStep: 134 \tLoss: 81.9410 \n",
            "Epoch: 1/5 \tStep: 135 \tLoss: 83.2713 \n",
            "Epoch: 1/5 \tStep: 136 \tLoss: 84.6519 \n",
            "Epoch: 1/5 \tStep: 137 \tLoss: 84.0676 \n",
            "Epoch: 1/5 \tStep: 138 \tLoss: 80.2178 \n",
            "Epoch: 1/5 \tStep: 139 \tLoss: 84.5153 \n",
            "Epoch: 1/5 \tStep: 140 \tLoss: 95.1689 \n",
            "Epoch: 1/5 \tStep: 141 \tLoss: 84.9930 \n",
            "Epoch: 1/5 \tStep: 142 \tLoss: 81.3950 \n",
            "Epoch: 1/5 \tStep: 143 \tLoss: 81.3736 \n",
            "Epoch: 1/5 \tStep: 144 \tLoss: 80.5830 \n",
            "Epoch: 1/5 \tStep: 145 \tLoss: 84.1279 \n",
            "Epoch: 1/5 \tStep: 146 \tLoss: 80.3217 \n",
            "Epoch: 1/5 \tStep: 147 \tLoss: 79.8846 \n",
            "Epoch: 1/5 \tStep: 148 \tLoss: 85.3682 \n",
            "Epoch: 1/5 \tStep: 149 \tLoss: 83.3945 \n",
            "Epoch: 1/5 \tStep: 150 \tLoss: 79.7372 \n",
            "Epoch: 1/5 \tStep: 151 \tLoss: 79.8860 \n",
            "Epoch: 1/5 \tStep: 152 \tLoss: 81.7004 \n",
            "Epoch: 1/5 \tStep: 153 \tLoss: 79.7938 \n",
            "Epoch: 1/5 \tStep: 154 \tLoss: 80.8735 \n",
            "Epoch: 1/5 \tStep: 155 \tLoss: 79.5087 \n",
            "Epoch: 1/5 \tStep: 156 \tLoss: 97.8847 \n",
            "Epoch: 1/5 \tStep: 157 \tLoss: 88.9154 \n",
            "Epoch: 1/5 \tStep: 158 \tLoss: 83.9336 \n",
            "Epoch: 1/5 \tStep: 159 \tLoss: 80.8789 \n",
            "Epoch: 1/5 \tStep: 160 \tLoss: 84.1715 \n",
            "Epoch: 1/5 \tStep: 161 \tLoss: 88.9571 \n",
            "Epoch: 1/5 \tStep: 162 \tLoss: 80.5366 \n",
            "Epoch: 1/5 \tStep: 163 \tLoss: 89.3804 \n",
            "Epoch: 1/5 \tStep: 164 \tLoss: 80.5792 \n",
            "Epoch: 1/5 \tStep: 165 \tLoss: 91.1853 \n",
            "Epoch: 1/5 \tStep: 166 \tLoss: 85.1569 \n",
            "Epoch: 1/5 \tStep: 167 \tLoss: 78.9990 \n",
            "Epoch: 1/5 \tStep: 168 \tLoss: 91.5147 \n",
            "Epoch: 1/5 \tStep: 169 \tLoss: 94.1845 \n",
            "Epoch: 1/5 \tStep: 170 \tLoss: 80.0049 \n",
            "Epoch: 1/5 \tStep: 171 \tLoss: 79.0879 \n",
            "Epoch: 1/5 \tStep: 172 \tLoss: 83.4752 \n",
            "Epoch: 1/5 \tStep: 173 \tLoss: 83.4491 \n",
            "Epoch: 1/5 \tStep: 174 \tLoss: 81.1417 \n",
            "Epoch: 1/5 \tStep: 175 \tLoss: 100.9379 \n",
            "Epoch: 1/5 \tStep: 176 \tLoss: 78.1534 \n",
            "Epoch: 1/5 \tStep: 177 \tLoss: 83.9807 \n",
            "Epoch: 1/5 \tStep: 178 \tLoss: 83.5435 \n",
            "Epoch: 1/5 \tStep: 179 \tLoss: 82.6709 \n",
            "Epoch: 1/5 \tStep: 180 \tLoss: 83.9936 \n",
            "Epoch: 1/5 \tStep: 181 \tLoss: 85.5772 \n",
            "Epoch: 1/5 \tStep: 182 \tLoss: 80.6474 \n",
            "Epoch: 1/5 \tStep: 183 \tLoss: 79.8094 \n",
            "Epoch: 1/5 \tStep: 184 \tLoss: 84.9535 \n",
            "Epoch: 1/5 \tStep: 185 \tLoss: 84.8803 \n",
            "Epoch: 1/5 \tStep: 186 \tLoss: 83.0249 \n",
            "Epoch: 1/5 \tStep: 187 \tLoss: 79.3812 \n",
            "Epoch: 1/5 \tStep: 188 \tLoss: 80.5524 \n",
            "Epoch: 1/5 \tStep: 189 \tLoss: 90.2193 \n",
            "Epoch: 1/5 \tStep: 190 \tLoss: 91.8810 \n",
            "Epoch: 1/5 \tStep: 191 \tLoss: 96.2992 \n",
            "Epoch: 1/5 \tStep: 192 \tLoss: 84.1157 \n",
            "Epoch: 1/5 \tStep: 193 \tLoss: 84.0660 \n",
            "Epoch: 1/5 \tStep: 194 \tLoss: 85.3917 \n",
            "Epoch: 1/5 \tStep: 195 \tLoss: 84.3169 \n",
            "Epoch: 1/5 \tStep: 196 \tLoss: 79.5070 \n",
            "Epoch: 1/5 \tStep: 197 \tLoss: 80.5396 \n",
            "Epoch: 1/5 \tStep: 198 \tLoss: 83.3616 \n",
            "Epoch: 1/5 \tStep: 199 \tLoss: 81.5478 \n",
            "Epoch: 1/5 \tStep: 200 \tLoss: 83.4927 \n",
            "Epoch: 1/5 \tStep: 201 \tLoss: 79.6738 \n",
            "Epoch: 1/5 \tStep: 202 \tLoss: 85.4837 \n",
            "Epoch: 1/5 \tStep: 203 \tLoss: 85.6676 \n",
            "Epoch: 1/5 \tStep: 204 \tLoss: 84.4552 \n",
            "Epoch: 1/5 \tStep: 205 \tLoss: 80.9164 \n",
            "Epoch: 1/5 \tStep: 206 \tLoss: 80.5894 \n",
            "Epoch: 1/5 \tStep: 207 \tLoss: 87.6863 \n",
            "Epoch: 1/5 \tStep: 208 \tLoss: 80.0123 \n",
            "Epoch: 1/5 \tStep: 209 \tLoss: 83.6451 \n",
            "Epoch: 1/5 \tStep: 210 \tLoss: 99.9367 \n",
            "Epoch: 1/5 \tStep: 211 \tLoss: 84.6616 \n",
            "Epoch: 1/5 \tStep: 212 \tLoss: 84.6926 \n",
            "Epoch: 1/5 \tStep: 213 \tLoss: 78.7289 \n",
            "Epoch: 1/5 \tStep: 214 \tLoss: 82.5465 \n",
            "Epoch: 1/5 \tStep: 215 \tLoss: 79.7818 \n",
            "Epoch: 1/5 \tStep: 216 \tLoss: 84.1448 \n",
            "Epoch: 1/5 \tStep: 217 \tLoss: 79.1059 \n",
            "Epoch: 1/5 \tStep: 218 \tLoss: 89.9404 \n",
            "Epoch: 1/5 \tStep: 219 \tLoss: 82.6200 \n",
            "Epoch: 1/5 \tStep: 220 \tLoss: 85.7662 \n",
            "Epoch: 1/5 \tStep: 221 \tLoss: 83.5447 \n",
            "Epoch: 1/5 \tStep: 222 \tLoss: 84.0666 \n",
            "Epoch: 1/5 \tStep: 223 \tLoss: 82.1661 \n",
            "Epoch: 1/5 \tStep: 224 \tLoss: 80.8228 \n",
            "Epoch: 1/5 \tStep: 225 \tLoss: 78.5726 \n",
            "Epoch: 1/5 \tStep: 226 \tLoss: 83.3379 \n",
            "Epoch: 1/5 \tStep: 227 \tLoss: 83.1291 \n",
            "Epoch: 1/5 \tStep: 228 \tLoss: 85.4620 \n",
            "Epoch: 1/5 \tStep: 229 \tLoss: 80.9080 \n",
            "Epoch: 1/5 \tStep: 230 \tLoss: 80.1869 \n",
            "Epoch: 1/5 \tStep: 231 \tLoss: 82.5813 \n",
            "Epoch: 1/5 \tStep: 232 \tLoss: 89.9716 \n",
            "Epoch: 1/5 \tStep: 233 \tLoss: 79.5311 \n",
            "Epoch: 1/5 \tStep: 234 \tLoss: 83.9343 \n",
            "Epoch: 1/5 \tStep: 235 \tLoss: 79.7838 \n",
            "Epoch: 1/5 \tStep: 236 \tLoss: 82.5784 \n",
            "Epoch: 1/5 \tStep: 237 \tLoss: 79.9501 \n",
            "Epoch: 1/5 \tStep: 238 \tLoss: 80.4051 \n",
            "Epoch: 1/5 \tStep: 239 \tLoss: 80.7130 \n",
            "Epoch: 1/5 \tStep: 240 \tLoss: 90.3100 \n",
            "Epoch: 1/5 \tStep: 241 \tLoss: 78.8915 \n",
            "Epoch: 1/5 \tStep: 242 \tLoss: 80.9526 \n",
            "Epoch: 1/5 \tStep: 243 \tLoss: 80.0463 \n",
            "Epoch: 1/5 \tStep: 244 \tLoss: 82.9115 \n",
            "Epoch: 1/5 \tStep: 245 \tLoss: 84.6330 \n",
            "Epoch: 1/5 \tStep: 246 \tLoss: 83.2482 \n",
            "Epoch: 1/5 \tStep: 247 \tLoss: 79.5547 \n",
            "Epoch: 1/5 \tStep: 248 \tLoss: 78.8127 \n",
            "Epoch: 1/5 \tStep: 249 \tLoss: 82.8366 \n",
            "Epoch: 1/5 \tStep: 250 \tLoss: 82.6645 \n",
            "Epoch: 1/5 \tStep: 251 \tLoss: 79.8420 \n",
            "Epoch: 1/5 \tStep: 252 \tLoss: 82.0331 \n",
            "Epoch: 1/5 \tStep: 253 \tLoss: 83.0866 \n",
            "Epoch: 1/5 \tStep: 254 \tLoss: 79.4313 \n",
            "Epoch: 1/5 \tStep: 255 \tLoss: 84.1160 \n",
            "Epoch: 1/5 \tStep: 256 \tLoss: 78.7093 \n",
            "Epoch: 1/5 \tStep: 257 \tLoss: 80.1116 \n",
            "Epoch: 1/5 \tStep: 258 \tLoss: 82.5005 \n",
            "Epoch: 1/5 \tStep: 259 \tLoss: 81.8469 \n",
            "Epoch: 1/5 \tStep: 260 \tLoss: 81.0123 \n",
            "Epoch: 1/5 \tStep: 261 \tLoss: 78.4627 \n",
            "Epoch: 1/5 \tStep: 262 \tLoss: 83.7672 \n",
            "Epoch: 1/5 \tStep: 263 \tLoss: 83.4246 \n",
            "Epoch: 1/5 \tStep: 264 \tLoss: 78.0743 \n",
            "Epoch: 1/5 \tStep: 265 \tLoss: 83.1219 \n",
            "Epoch: 1/5 \tStep: 266 \tLoss: 82.5867 \n",
            "Epoch: 1/5 \tStep: 267 \tLoss: 83.4658 \n",
            "Epoch: 1/5 \tStep: 268 \tLoss: 78.8446 \n",
            "Epoch: 1/5 \tStep: 269 \tLoss: 96.1384 \n",
            "Epoch: 1/5 \tStep: 270 \tLoss: 79.2909 \n",
            "Epoch: 1/5 \tStep: 271 \tLoss: 82.1364 \n",
            "Epoch: 1/5 \tStep: 272 \tLoss: 79.0112 \n",
            "Epoch: 1/5 \tStep: 273 \tLoss: 82.6732 \n",
            "Epoch: 1/5 \tStep: 274 \tLoss: 82.4573 \n",
            "Epoch: 1/5 \tStep: 275 \tLoss: 81.8672 \n",
            "Epoch: 1/5 \tStep: 276 \tLoss: 82.0871 \n",
            "Epoch: 1/5 \tStep: 277 \tLoss: 83.4219 \n",
            "Epoch: 1/5 \tStep: 278 \tLoss: 83.6194 \n",
            "Epoch: 1/5 \tStep: 279 \tLoss: 79.5323 \n",
            "Epoch: 1/5 \tStep: 280 \tLoss: 79.1888 \n",
            "Epoch: 1/5 \tStep: 281 \tLoss: 89.3249 \n",
            "Epoch: 1/5 \tStep: 282 \tLoss: 78.3365 \n",
            "Epoch: 1/5 \tStep: 283 \tLoss: 86.2612 \n",
            "Epoch: 1/5 \tStep: 284 \tLoss: 88.9955 \n",
            "Epoch: 2/5 \tStep: 285 \tLoss: 84.9880 \n",
            "Epoch: 2/5 \tStep: 286 \tLoss: 82.8742 \n",
            "Epoch: 2/5 \tStep: 287 \tLoss: 80.7553 \n",
            "Epoch: 2/5 \tStep: 288 \tLoss: 84.2924 \n",
            "Epoch: 2/5 \tStep: 289 \tLoss: 82.7280 \n",
            "Epoch: 2/5 \tStep: 290 \tLoss: 80.1440 \n",
            "Epoch: 2/5 \tStep: 291 \tLoss: 83.2674 \n",
            "Epoch: 2/5 \tStep: 292 \tLoss: 78.5285 \n",
            "Epoch: 2/5 \tStep: 293 \tLoss: 82.5668 \n",
            "Epoch: 2/5 \tStep: 294 \tLoss: 88.5773 \n",
            "Epoch: 2/5 \tStep: 295 \tLoss: 82.0640 \n",
            "Epoch: 2/5 \tStep: 296 \tLoss: 78.1294 \n",
            "Epoch: 2/5 \tStep: 297 \tLoss: 89.4796 \n",
            "Epoch: 2/5 \tStep: 298 \tLoss: 78.5103 \n",
            "Epoch: 2/5 \tStep: 299 \tLoss: 82.2093 \n",
            "Epoch: 2/5 \tStep: 300 \tLoss: 78.8294 \n",
            "Epoch: 2/5 \tStep: 301 \tLoss: 79.1506 \n",
            "Epoch: 2/5 \tStep: 302 \tLoss: 87.6132 \n",
            "Epoch: 2/5 \tStep: 303 \tLoss: 82.7929 \n",
            "Epoch: 2/5 \tStep: 304 \tLoss: 83.8332 \n",
            "Epoch: 2/5 \tStep: 305 \tLoss: 81.6385 \n",
            "Epoch: 2/5 \tStep: 306 \tLoss: 80.0956 \n",
            "Epoch: 2/5 \tStep: 307 \tLoss: 89.5856 \n",
            "Epoch: 2/5 \tStep: 308 \tLoss: 83.1244 \n",
            "Epoch: 2/5 \tStep: 309 \tLoss: 82.7983 \n",
            "Epoch: 2/5 \tStep: 310 \tLoss: 83.7673 \n",
            "Epoch: 2/5 \tStep: 311 \tLoss: 83.5033 \n",
            "Epoch: 2/5 \tStep: 312 \tLoss: 82.2249 \n",
            "Epoch: 2/5 \tStep: 313 \tLoss: 81.9165 \n",
            "Epoch: 2/5 \tStep: 314 \tLoss: 97.5908 \n",
            "Epoch: 2/5 \tStep: 315 \tLoss: 87.0847 \n",
            "Epoch: 2/5 \tStep: 316 \tLoss: 88.5690 \n",
            "Epoch: 2/5 \tStep: 317 \tLoss: 85.0364 \n",
            "Epoch: 2/5 \tStep: 318 \tLoss: 84.5245 \n",
            "Epoch: 2/5 \tStep: 319 \tLoss: 78.9342 \n",
            "Epoch: 2/5 \tStep: 320 \tLoss: 83.6744 \n",
            "Epoch: 2/5 \tStep: 321 \tLoss: 82.1001 \n",
            "Epoch: 2/5 \tStep: 322 \tLoss: 79.9343 \n",
            "Epoch: 2/5 \tStep: 323 \tLoss: 91.0802 \n",
            "Epoch: 2/5 \tStep: 324 \tLoss: 78.2450 \n",
            "Epoch: 2/5 \tStep: 325 \tLoss: 82.8865 \n",
            "Epoch: 2/5 \tStep: 326 \tLoss: 83.9690 \n",
            "Epoch: 2/5 \tStep: 327 \tLoss: 79.2176 \n",
            "Epoch: 2/5 \tStep: 328 \tLoss: 78.5841 \n",
            "Epoch: 2/5 \tStep: 329 \tLoss: 78.5314 \n",
            "Epoch: 2/5 \tStep: 330 \tLoss: 94.3206 \n",
            "Epoch: 2/5 \tStep: 331 \tLoss: 81.4293 \n",
            "Epoch: 2/5 \tStep: 332 \tLoss: 81.6244 \n",
            "Epoch: 2/5 \tStep: 333 \tLoss: 77.6075 \n",
            "Epoch: 2/5 \tStep: 334 \tLoss: 87.0549 \n",
            "Epoch: 2/5 \tStep: 335 \tLoss: 79.3057 \n",
            "Epoch: 2/5 \tStep: 336 \tLoss: 80.1674 \n",
            "Epoch: 2/5 \tStep: 337 \tLoss: 87.4110 \n",
            "Epoch: 2/5 \tStep: 338 \tLoss: 82.6784 \n",
            "Epoch: 2/5 \tStep: 339 \tLoss: 82.5798 \n",
            "Epoch: 2/5 \tStep: 340 \tLoss: 78.7076 \n",
            "Epoch: 2/5 \tStep: 341 \tLoss: 83.7404 \n",
            "Epoch: 2/5 \tStep: 342 \tLoss: 82.5865 \n",
            "Epoch: 2/5 \tStep: 343 \tLoss: 92.4286 \n",
            "Epoch: 2/5 \tStep: 344 \tLoss: 82.5216 \n",
            "Epoch: 2/5 \tStep: 345 \tLoss: 82.3987 \n",
            "Epoch: 2/5 \tStep: 346 \tLoss: 78.0796 \n",
            "Epoch: 2/5 \tStep: 347 \tLoss: 82.5337 \n",
            "Epoch: 2/5 \tStep: 348 \tLoss: 82.4640 \n",
            "Epoch: 2/5 \tStep: 349 \tLoss: 83.0696 \n",
            "Epoch: 2/5 \tStep: 350 \tLoss: 77.4928 \n",
            "Epoch: 2/5 \tStep: 351 \tLoss: 82.0999 \n",
            "Epoch: 2/5 \tStep: 352 \tLoss: 78.5044 \n",
            "Epoch: 2/5 \tStep: 353 \tLoss: 78.7952 \n",
            "Epoch: 2/5 \tStep: 354 \tLoss: 81.7545 \n",
            "Epoch: 2/5 \tStep: 355 \tLoss: 78.5122 \n",
            "Epoch: 2/5 \tStep: 356 \tLoss: 80.1845 \n",
            "Epoch: 2/5 \tStep: 357 \tLoss: 81.2465 \n",
            "Epoch: 2/5 \tStep: 358 \tLoss: 81.5465 \n",
            "Epoch: 2/5 \tStep: 359 \tLoss: 82.4072 \n",
            "Epoch: 2/5 \tStep: 360 \tLoss: 95.5593 \n",
            "Epoch: 2/5 \tStep: 361 \tLoss: 78.8232 \n",
            "Epoch: 2/5 \tStep: 362 \tLoss: 82.1973 \n",
            "Epoch: 2/5 \tStep: 363 \tLoss: 81.9794 \n",
            "Epoch: 2/5 \tStep: 364 \tLoss: 80.4773 \n",
            "Epoch: 2/5 \tStep: 365 \tLoss: 77.7763 \n",
            "Epoch: 2/5 \tStep: 366 \tLoss: 82.2397 \n",
            "Epoch: 2/5 \tStep: 367 \tLoss: 86.7298 \n",
            "Epoch: 2/5 \tStep: 368 \tLoss: 78.2520 \n",
            "Epoch: 2/5 \tStep: 369 \tLoss: 82.5390 \n",
            "Epoch: 2/5 \tStep: 370 \tLoss: 81.5649 \n",
            "Epoch: 2/5 \tStep: 371 \tLoss: 83.2992 \n",
            "Epoch: 2/5 \tStep: 372 \tLoss: 78.9943 \n",
            "Epoch: 2/5 \tStep: 373 \tLoss: 82.9931 \n",
            "Epoch: 2/5 \tStep: 374 \tLoss: 83.0307 \n",
            "Epoch: 2/5 \tStep: 375 \tLoss: 78.6393 \n",
            "Epoch: 2/5 \tStep: 376 \tLoss: 81.1896 \n",
            "Epoch: 2/5 \tStep: 377 \tLoss: 78.4099 \n",
            "Epoch: 2/5 \tStep: 378 \tLoss: 78.2250 \n",
            "Epoch: 2/5 \tStep: 379 \tLoss: 81.8744 \n",
            "Epoch: 2/5 \tStep: 380 \tLoss: 80.8799 \n",
            "Epoch: 2/5 \tStep: 381 \tLoss: 94.5021 \n",
            "Epoch: 2/5 \tStep: 382 \tLoss: 81.4451 \n",
            "Epoch: 2/5 \tStep: 383 \tLoss: 77.8221 \n",
            "Epoch: 2/5 \tStep: 384 \tLoss: 78.4514 \n",
            "Epoch: 2/5 \tStep: 385 \tLoss: 82.4779 \n",
            "Epoch: 2/5 \tStep: 386 \tLoss: 81.9460 \n",
            "Epoch: 2/5 \tStep: 387 \tLoss: 77.6359 \n",
            "Epoch: 2/5 \tStep: 388 \tLoss: 84.5233 \n",
            "Epoch: 2/5 \tStep: 389 \tLoss: 77.6056 \n",
            "Epoch: 2/5 \tStep: 390 \tLoss: 81.6745 \n",
            "Epoch: 2/5 \tStep: 391 \tLoss: 78.2438 \n",
            "Epoch: 2/5 \tStep: 392 \tLoss: 89.1122 \n",
            "Epoch: 2/5 \tStep: 393 \tLoss: 82.2996 \n",
            "Epoch: 2/5 \tStep: 394 \tLoss: 80.0573 \n",
            "Epoch: 2/5 \tStep: 395 \tLoss: 83.0651 \n",
            "Epoch: 2/5 \tStep: 396 \tLoss: 81.9054 \n",
            "Epoch: 2/5 \tStep: 397 \tLoss: 78.9973 \n",
            "Epoch: 2/5 \tStep: 398 \tLoss: 81.2166 \n",
            "Epoch: 2/5 \tStep: 399 \tLoss: 82.6151 \n",
            "Epoch: 2/5 \tStep: 400 \tLoss: 78.5335 \n",
            "Epoch: 2/5 \tStep: 401 \tLoss: 82.1391 \n",
            "Epoch: 2/5 \tStep: 402 \tLoss: 78.7875 \n",
            "Epoch: 2/5 \tStep: 403 \tLoss: 80.7234 \n",
            "Epoch: 2/5 \tStep: 404 \tLoss: 78.7643 \n",
            "Epoch: 2/5 \tStep: 405 \tLoss: 78.4604 \n",
            "Epoch: 2/5 \tStep: 406 \tLoss: 78.4010 \n",
            "Epoch: 2/5 \tStep: 407 \tLoss: 80.9944 \n",
            "Epoch: 2/5 \tStep: 408 \tLoss: 81.8646 \n",
            "Epoch: 2/5 \tStep: 409 \tLoss: 77.2062 \n",
            "Epoch: 2/5 \tStep: 410 \tLoss: 78.6219 \n",
            "Epoch: 2/5 \tStep: 411 \tLoss: 82.8294 \n",
            "Epoch: 2/5 \tStep: 412 \tLoss: 88.6348 \n",
            "Epoch: 2/5 \tStep: 413 \tLoss: 77.7434 \n",
            "Epoch: 2/5 \tStep: 414 \tLoss: 78.2533 \n",
            "Epoch: 2/5 \tStep: 415 \tLoss: 81.9503 \n",
            "Epoch: 2/5 \tStep: 416 \tLoss: 87.0942 \n",
            "Epoch: 2/5 \tStep: 417 \tLoss: 78.6102 \n",
            "Epoch: 2/5 \tStep: 418 \tLoss: 80.5766 \n",
            "Epoch: 2/5 \tStep: 419 \tLoss: 82.6843 \n",
            "Epoch: 2/5 \tStep: 420 \tLoss: 87.3170 \n",
            "Epoch: 2/5 \tStep: 421 \tLoss: 81.7124 \n",
            "Epoch: 2/5 \tStep: 422 \tLoss: 78.0837 \n",
            "Epoch: 2/5 \tStep: 423 \tLoss: 81.3347 \n",
            "Epoch: 2/5 \tStep: 424 \tLoss: 82.4898 \n",
            "Epoch: 2/5 \tStep: 425 \tLoss: 97.8494 \n",
            "Epoch: 2/5 \tStep: 426 \tLoss: 77.5895 \n",
            "Epoch: 2/5 \tStep: 427 \tLoss: 83.3995 \n",
            "Epoch: 2/5 \tStep: 428 \tLoss: 77.8424 \n",
            "Epoch: 2/5 \tStep: 429 \tLoss: 80.9681 \n",
            "Epoch: 2/5 \tStep: 430 \tLoss: 78.6448 \n",
            "Epoch: 2/5 \tStep: 431 \tLoss: 81.7459 \n",
            "Epoch: 2/5 \tStep: 432 \tLoss: 78.3140 \n",
            "Epoch: 2/5 \tStep: 433 \tLoss: 80.7468 \n",
            "Epoch: 2/5 \tStep: 434 \tLoss: 89.1813 \n",
            "Epoch: 2/5 \tStep: 435 \tLoss: 81.0590 \n",
            "Epoch: 2/5 \tStep: 436 \tLoss: 78.1045 \n",
            "Epoch: 2/5 \tStep: 437 \tLoss: 77.3692 \n",
            "Epoch: 2/5 \tStep: 438 \tLoss: 82.3203 \n",
            "Epoch: 2/5 \tStep: 439 \tLoss: 82.4030 \n",
            "Epoch: 2/5 \tStep: 440 \tLoss: 78.5307 \n",
            "Epoch: 2/5 \tStep: 441 \tLoss: 77.2655 \n",
            "Epoch: 2/5 \tStep: 442 \tLoss: 85.8420 \n",
            "Epoch: 2/5 \tStep: 443 \tLoss: 81.9704 \n",
            "Epoch: 2/5 \tStep: 444 \tLoss: 81.2099 \n",
            "Epoch: 2/5 \tStep: 445 \tLoss: 81.7398 \n",
            "Epoch: 2/5 \tStep: 446 \tLoss: 81.8754 \n",
            "Epoch: 2/5 \tStep: 447 \tLoss: 81.8013 \n",
            "Epoch: 2/5 \tStep: 448 \tLoss: 77.8570 \n",
            "Epoch: 2/5 \tStep: 449 \tLoss: 77.3341 \n",
            "Epoch: 2/5 \tStep: 450 \tLoss: 77.2564 \n",
            "Epoch: 2/5 \tStep: 451 \tLoss: 81.9327 \n",
            "Epoch: 2/5 \tStep: 452 \tLoss: 80.7667 \n",
            "Epoch: 2/5 \tStep: 453 \tLoss: 82.3569 \n",
            "Epoch: 2/5 \tStep: 454 \tLoss: 77.2837 \n",
            "Epoch: 2/5 \tStep: 455 \tLoss: 76.8306 \n",
            "Epoch: 2/5 \tStep: 456 \tLoss: 80.3368 \n",
            "Epoch: 2/5 \tStep: 457 \tLoss: 82.0782 \n",
            "Epoch: 2/5 \tStep: 458 \tLoss: 78.5001 \n",
            "Epoch: 2/5 \tStep: 459 \tLoss: 81.2016 \n",
            "Epoch: 2/5 \tStep: 460 \tLoss: 81.7085 \n",
            "Epoch: 2/5 \tStep: 461 \tLoss: 76.3660 \n",
            "Epoch: 2/5 \tStep: 462 \tLoss: 77.0651 \n",
            "Epoch: 2/5 \tStep: 463 \tLoss: 77.3568 \n",
            "Epoch: 2/5 \tStep: 464 \tLoss: 77.5393 \n",
            "Epoch: 2/5 \tStep: 465 \tLoss: 77.8514 \n",
            "Epoch: 2/5 \tStep: 466 \tLoss: 90.7185 \n",
            "Epoch: 2/5 \tStep: 467 \tLoss: 78.2646 \n",
            "Epoch: 2/5 \tStep: 468 \tLoss: 80.5156 \n",
            "Epoch: 2/5 \tStep: 469 \tLoss: 81.9803 \n",
            "Epoch: 2/5 \tStep: 470 \tLoss: 78.7781 \n",
            "Epoch: 2/5 \tStep: 471 \tLoss: 82.8595 \n",
            "Epoch: 2/5 \tStep: 472 \tLoss: 77.2064 \n",
            "Epoch: 2/5 \tStep: 473 \tLoss: 82.1601 \n",
            "Epoch: 2/5 \tStep: 474 \tLoss: 87.3694 \n",
            "Epoch: 2/5 \tStep: 475 \tLoss: 77.1566 \n",
            "Epoch: 2/5 \tStep: 476 \tLoss: 86.2798 \n",
            "Epoch: 2/5 \tStep: 477 \tLoss: 85.5583 \n",
            "Epoch: 2/5 \tStep: 478 \tLoss: 77.8303 \n",
            "Epoch: 2/5 \tStep: 479 \tLoss: 82.1451 \n",
            "Epoch: 2/5 \tStep: 480 \tLoss: 80.4836 \n",
            "Epoch: 2/5 \tStep: 481 \tLoss: 78.8647 \n",
            "Epoch: 2/5 \tStep: 482 \tLoss: 81.0607 \n",
            "Epoch: 2/5 \tStep: 483 \tLoss: 83.8612 \n",
            "Epoch: 2/5 \tStep: 484 \tLoss: 80.7333 \n",
            "Epoch: 2/5 \tStep: 485 \tLoss: 80.7879 \n",
            "Epoch: 2/5 \tStep: 486 \tLoss: 83.0912 \n",
            "Epoch: 2/5 \tStep: 487 \tLoss: 81.1875 \n",
            "Epoch: 2/5 \tStep: 488 \tLoss: 94.2145 \n",
            "Epoch: 2/5 \tStep: 489 \tLoss: 79.7416 \n",
            "Epoch: 2/5 \tStep: 490 \tLoss: 77.4750 \n",
            "Epoch: 2/5 \tStep: 491 \tLoss: 81.1194 \n",
            "Epoch: 2/5 \tStep: 492 \tLoss: 77.7747 \n",
            "Epoch: 2/5 \tStep: 493 \tLoss: 81.7731 \n",
            "Epoch: 2/5 \tStep: 494 \tLoss: 77.7892 \n",
            "Epoch: 2/5 \tStep: 495 \tLoss: 82.2246 \n",
            "Epoch: 2/5 \tStep: 496 \tLoss: 81.8106 \n",
            "Epoch: 2/5 \tStep: 497 \tLoss: 82.5402 \n",
            "Epoch: 2/5 \tStep: 498 \tLoss: 77.0786 \n",
            "Epoch: 2/5 \tStep: 499 \tLoss: 81.6022 \n",
            "Epoch: 2/5 \tStep: 500 \tLoss: 80.9246 \n",
            "Epoch: 2/5 \tStep: 501 \tLoss: 80.5965 \n",
            "Epoch: 2/5 \tStep: 502 \tLoss: 81.4521 \n",
            "Epoch: 2/5 \tStep: 503 \tLoss: 77.7832 \n",
            "Epoch: 2/5 \tStep: 504 \tLoss: 77.3219 \n",
            "Epoch: 2/5 \tStep: 505 \tLoss: 81.9287 \n",
            "Epoch: 2/5 \tStep: 506 \tLoss: 89.0145 \n",
            "Epoch: 2/5 \tStep: 507 \tLoss: 84.4493 \n",
            "Epoch: 2/5 \tStep: 508 \tLoss: 86.5899 \n",
            "Epoch: 2/5 \tStep: 509 \tLoss: 82.5346 \n",
            "Epoch: 2/5 \tStep: 510 \tLoss: 80.7081 \n",
            "Epoch: 2/5 \tStep: 511 \tLoss: 77.8458 \n",
            "Epoch: 2/5 \tStep: 512 \tLoss: 77.8865 \n",
            "Epoch: 2/5 \tStep: 513 \tLoss: 80.9262 \n",
            "Epoch: 2/5 \tStep: 514 \tLoss: 78.5879 \n",
            "Epoch: 2/5 \tStep: 515 \tLoss: 80.9937 \n",
            "Epoch: 2/5 \tStep: 516 \tLoss: 81.0856 \n",
            "Epoch: 2/5 \tStep: 517 \tLoss: 77.3150 \n",
            "Epoch: 2/5 \tStep: 518 \tLoss: 80.1765 \n",
            "Epoch: 2/5 \tStep: 519 \tLoss: 82.8081 \n",
            "Epoch: 2/5 \tStep: 520 \tLoss: 83.3184 \n",
            "Epoch: 2/5 \tStep: 521 \tLoss: 81.8292 \n",
            "Epoch: 2/5 \tStep: 522 \tLoss: 77.4421 \n",
            "Epoch: 2/5 \tStep: 523 \tLoss: 84.7138 \n",
            "Epoch: 2/5 \tStep: 524 \tLoss: 82.3550 \n",
            "Epoch: 2/5 \tStep: 525 \tLoss: 77.9816 \n",
            "Epoch: 2/5 \tStep: 526 \tLoss: 76.8860 \n",
            "Epoch: 2/5 \tStep: 527 \tLoss: 80.5577 \n",
            "Epoch: 2/5 \tStep: 528 \tLoss: 90.9109 \n",
            "Epoch: 2/5 \tStep: 529 \tLoss: 82.1601 \n",
            "Epoch: 2/5 \tStep: 530 \tLoss: 82.7830 \n",
            "Epoch: 2/5 \tStep: 531 \tLoss: 81.1524 \n",
            "Epoch: 2/5 \tStep: 532 \tLoss: 75.7407 \n",
            "Epoch: 2/5 \tStep: 533 \tLoss: 82.1475 \n",
            "Epoch: 2/5 \tStep: 534 \tLoss: 82.2185 \n",
            "Epoch: 2/5 \tStep: 535 \tLoss: 81.5668 \n",
            "Epoch: 2/5 \tStep: 536 \tLoss: 80.9779 \n",
            "Epoch: 2/5 \tStep: 537 \tLoss: 83.7824 \n",
            "Epoch: 2/5 \tStep: 538 \tLoss: 81.9873 \n",
            "Epoch: 2/5 \tStep: 539 \tLoss: 82.5079 \n",
            "Epoch: 2/5 \tStep: 540 \tLoss: 81.7291 \n",
            "Epoch: 2/5 \tStep: 541 \tLoss: 82.5880 \n",
            "Epoch: 2/5 \tStep: 542 \tLoss: 80.8091 \n",
            "Epoch: 2/5 \tStep: 543 \tLoss: 76.6726 \n",
            "Epoch: 2/5 \tStep: 544 \tLoss: 80.7598 \n",
            "Epoch: 2/5 \tStep: 545 \tLoss: 82.3381 \n",
            "Epoch: 2/5 \tStep: 546 \tLoss: 77.9685 \n",
            "Epoch: 2/5 \tStep: 547 \tLoss: 82.1914 \n",
            "Epoch: 2/5 \tStep: 548 \tLoss: 80.4693 \n",
            "Epoch: 2/5 \tStep: 549 \tLoss: 86.9755 \n",
            "Epoch: 2/5 \tStep: 550 \tLoss: 81.7920 \n",
            "Epoch: 2/5 \tStep: 551 \tLoss: 83.1623 \n",
            "Epoch: 2/5 \tStep: 552 \tLoss: 79.0527 \n",
            "Epoch: 2/5 \tStep: 553 \tLoss: 80.9289 \n",
            "Epoch: 2/5 \tStep: 554 \tLoss: 81.1804 \n",
            "Epoch: 2/5 \tStep: 555 \tLoss: 87.1537 \n",
            "Epoch: 2/5 \tStep: 556 \tLoss: 80.9535 \n",
            "Epoch: 2/5 \tStep: 557 \tLoss: 80.8057 \n",
            "Epoch: 2/5 \tStep: 558 \tLoss: 78.7287 \n",
            "Epoch: 2/5 \tStep: 559 \tLoss: 78.1011 \n",
            "Epoch: 2/5 \tStep: 560 \tLoss: 83.0149 \n",
            "Epoch: 2/5 \tStep: 561 \tLoss: 77.3198 \n",
            "Epoch: 2/5 \tStep: 562 \tLoss: 77.4501 \n",
            "Epoch: 2/5 \tStep: 563 \tLoss: 82.0307 \n",
            "Epoch: 2/5 \tStep: 564 \tLoss: 82.3036 \n",
            "Epoch: 2/5 \tStep: 565 \tLoss: 81.9250 \n",
            "Epoch: 2/5 \tStep: 566 \tLoss: 77.9193 \n",
            "Epoch: 2/5 \tStep: 567 \tLoss: 79.2764 \n",
            "Epoch: 2/5 \tStep: 568 \tLoss: 78.8646 \n",
            "Epoch: 2/5 \tStep: 569 \tLoss: 83.0026 \n",
            "Epoch: 3/5 \tStep: 570 \tLoss: 86.3448 \n",
            "Epoch: 3/5 \tStep: 571 \tLoss: 76.5090 \n",
            "Epoch: 3/5 \tStep: 572 \tLoss: 81.0518 \n",
            "Epoch: 3/5 \tStep: 573 \tLoss: 82.1836 \n",
            "Epoch: 3/5 \tStep: 574 \tLoss: 91.7659 \n",
            "Epoch: 3/5 \tStep: 575 \tLoss: 81.0115 \n",
            "Epoch: 3/5 \tStep: 576 \tLoss: 80.6119 \n",
            "Epoch: 3/5 \tStep: 577 \tLoss: 76.9381 \n",
            "Epoch: 3/5 \tStep: 578 \tLoss: 79.9577 \n",
            "Epoch: 3/5 \tStep: 579 \tLoss: 80.8420 \n",
            "Epoch: 3/5 \tStep: 580 \tLoss: 81.2779 \n",
            "Epoch: 3/5 \tStep: 581 \tLoss: 82.1252 \n",
            "Epoch: 3/5 \tStep: 582 \tLoss: 81.5570 \n",
            "Epoch: 3/5 \tStep: 583 \tLoss: 81.5315 \n",
            "Epoch: 3/5 \tStep: 584 \tLoss: 75.6813 \n",
            "Epoch: 3/5 \tStep: 585 \tLoss: 99.7415 \n",
            "Epoch: 3/5 \tStep: 586 \tLoss: 82.6282 \n",
            "Epoch: 3/5 \tStep: 587 \tLoss: 82.1246 \n",
            "Epoch: 3/5 \tStep: 588 \tLoss: 86.9840 \n",
            "Epoch: 3/5 \tStep: 589 \tLoss: 78.6765 \n",
            "Epoch: 3/5 \tStep: 590 \tLoss: 83.2976 \n",
            "Epoch: 3/5 \tStep: 591 \tLoss: 82.6342 \n",
            "Epoch: 3/5 \tStep: 592 \tLoss: 89.6426 \n",
            "Epoch: 3/5 \tStep: 593 \tLoss: 77.9668 \n",
            "Epoch: 3/5 \tStep: 594 \tLoss: 77.6589 \n",
            "Epoch: 3/5 \tStep: 595 \tLoss: 80.0621 \n",
            "Epoch: 3/5 \tStep: 596 \tLoss: 77.5933 \n",
            "Epoch: 3/5 \tStep: 597 \tLoss: 77.7809 \n",
            "Epoch: 3/5 \tStep: 598 \tLoss: 81.8730 \n",
            "Epoch: 3/5 \tStep: 599 \tLoss: 77.8127 \n",
            "Epoch: 3/5 \tStep: 600 \tLoss: 86.9103 \n",
            "Epoch: 3/5 \tStep: 601 \tLoss: 79.9411 \n",
            "Epoch: 3/5 \tStep: 602 \tLoss: 80.9190 \n",
            "Epoch: 3/5 \tStep: 603 \tLoss: 79.0134 \n",
            "Epoch: 3/5 \tStep: 604 \tLoss: 77.2186 \n",
            "Epoch: 3/5 \tStep: 605 \tLoss: 77.4481 \n",
            "Epoch: 3/5 \tStep: 606 \tLoss: 78.7355 \n",
            "Epoch: 3/5 \tStep: 607 \tLoss: 76.0849 \n",
            "Epoch: 3/5 \tStep: 608 \tLoss: 77.6360 \n",
            "Epoch: 3/5 \tStep: 609 \tLoss: 77.3205 \n",
            "Epoch: 3/5 \tStep: 610 \tLoss: 82.7057 \n",
            "Epoch: 3/5 \tStep: 611 \tLoss: 81.2786 \n",
            "Epoch: 3/5 \tStep: 612 \tLoss: 81.4721 \n",
            "Epoch: 3/5 \tStep: 613 \tLoss: 81.4621 \n",
            "Epoch: 3/5 \tStep: 614 \tLoss: 77.6172 \n",
            "Epoch: 3/5 \tStep: 615 \tLoss: 79.9306 \n",
            "Epoch: 3/5 \tStep: 616 \tLoss: 82.1312 \n",
            "Epoch: 3/5 \tStep: 617 \tLoss: 81.9170 \n",
            "Epoch: 3/5 \tStep: 618 \tLoss: 77.6469 \n",
            "Epoch: 3/5 \tStep: 619 \tLoss: 77.1505 \n",
            "Epoch: 3/5 \tStep: 620 \tLoss: 78.8163 \n",
            "Epoch: 3/5 \tStep: 621 \tLoss: 79.8124 \n",
            "Epoch: 3/5 \tStep: 622 \tLoss: 78.1052 \n",
            "Epoch: 3/5 \tStep: 623 \tLoss: 80.5836 \n",
            "Epoch: 3/5 \tStep: 624 \tLoss: 81.7347 \n",
            "Epoch: 3/5 \tStep: 625 \tLoss: 83.4810 \n",
            "Epoch: 3/5 \tStep: 626 \tLoss: 79.0759 \n",
            "Epoch: 3/5 \tStep: 627 \tLoss: 81.8148 \n",
            "Epoch: 3/5 \tStep: 628 \tLoss: 83.0592 \n",
            "Epoch: 3/5 \tStep: 629 \tLoss: 90.2981 \n",
            "Epoch: 3/5 \tStep: 630 \tLoss: 81.5845 \n",
            "Epoch: 3/5 \tStep: 631 \tLoss: 81.4402 \n",
            "Epoch: 3/5 \tStep: 632 \tLoss: 80.4396 \n",
            "Epoch: 3/5 \tStep: 633 \tLoss: 88.3196 \n",
            "Epoch: 3/5 \tStep: 634 \tLoss: 76.8819 \n",
            "Epoch: 3/5 \tStep: 635 \tLoss: 81.6029 \n",
            "Epoch: 3/5 \tStep: 636 \tLoss: 80.5668 \n",
            "Epoch: 3/5 \tStep: 637 \tLoss: 80.0937 \n",
            "Epoch: 3/5 \tStep: 638 \tLoss: 77.6948 \n",
            "Epoch: 3/5 \tStep: 639 \tLoss: 79.9385 \n",
            "Epoch: 3/5 \tStep: 640 \tLoss: 77.8626 \n",
            "Epoch: 3/5 \tStep: 641 \tLoss: 77.2044 \n",
            "Epoch: 3/5 \tStep: 642 \tLoss: 77.8267 \n",
            "Epoch: 3/5 \tStep: 643 \tLoss: 77.8975 \n",
            "Epoch: 3/5 \tStep: 644 \tLoss: 80.5598 \n",
            "Epoch: 3/5 \tStep: 645 \tLoss: 80.4900 \n",
            "Epoch: 3/5 \tStep: 646 \tLoss: 81.1456 \n",
            "Epoch: 3/5 \tStep: 647 \tLoss: 81.3097 \n",
            "Epoch: 3/5 \tStep: 648 \tLoss: 80.6329 \n",
            "Epoch: 3/5 \tStep: 649 \tLoss: 83.5503 \n",
            "Epoch: 3/5 \tStep: 650 \tLoss: 75.7997 \n",
            "Epoch: 3/5 \tStep: 651 \tLoss: 77.9542 \n",
            "Epoch: 3/5 \tStep: 652 \tLoss: 82.0378 \n",
            "Epoch: 3/5 \tStep: 653 \tLoss: 76.9457 \n",
            "Epoch: 3/5 \tStep: 654 \tLoss: 82.2186 \n",
            "Epoch: 3/5 \tStep: 655 \tLoss: 81.4151 \n",
            "Epoch: 3/5 \tStep: 656 \tLoss: 77.8364 \n",
            "Epoch: 3/5 \tStep: 657 \tLoss: 77.7017 \n",
            "Epoch: 3/5 \tStep: 658 \tLoss: 86.2948 \n",
            "Epoch: 3/5 \tStep: 659 \tLoss: 79.6746 \n",
            "Epoch: 3/5 \tStep: 660 \tLoss: 80.8578 \n",
            "Epoch: 3/5 \tStep: 661 \tLoss: 80.3923 \n",
            "Epoch: 3/5 \tStep: 662 \tLoss: 86.9323 \n",
            "Epoch: 3/5 \tStep: 663 \tLoss: 81.0805 \n",
            "Epoch: 3/5 \tStep: 664 \tLoss: 84.0983 \n",
            "Epoch: 3/5 \tStep: 665 \tLoss: 81.8056 \n",
            "Epoch: 3/5 \tStep: 666 \tLoss: 77.2799 \n",
            "Epoch: 3/5 \tStep: 667 \tLoss: 75.8949 \n",
            "Epoch: 3/5 \tStep: 668 \tLoss: 78.0055 \n",
            "Epoch: 3/5 \tStep: 669 \tLoss: 85.8536 \n",
            "Epoch: 3/5 \tStep: 670 \tLoss: 76.1146 \n",
            "Epoch: 3/5 \tStep: 671 \tLoss: 76.5669 \n",
            "Epoch: 3/5 \tStep: 672 \tLoss: 81.6165 \n",
            "Epoch: 3/5 \tStep: 673 \tLoss: 75.0990 \n",
            "Epoch: 3/5 \tStep: 674 \tLoss: 96.2401 \n",
            "Epoch: 3/5 \tStep: 675 \tLoss: 79.5996 \n",
            "Epoch: 3/5 \tStep: 676 \tLoss: 79.9297 \n",
            "Epoch: 3/5 \tStep: 677 \tLoss: 81.3255 \n",
            "Epoch: 3/5 \tStep: 678 \tLoss: 77.5469 \n",
            "Epoch: 3/5 \tStep: 679 \tLoss: 82.3183 \n",
            "Epoch: 3/5 \tStep: 680 \tLoss: 80.0521 \n",
            "Epoch: 3/5 \tStep: 681 \tLoss: 80.7573 \n",
            "Epoch: 3/5 \tStep: 682 \tLoss: 81.8468 \n",
            "Epoch: 3/5 \tStep: 683 \tLoss: 90.5181 \n",
            "Epoch: 3/5 \tStep: 684 \tLoss: 77.1246 \n",
            "Epoch: 3/5 \tStep: 685 \tLoss: 81.5297 \n",
            "Epoch: 3/5 \tStep: 686 \tLoss: 81.1377 \n",
            "Epoch: 3/5 \tStep: 687 \tLoss: 81.8676 \n",
            "Epoch: 3/5 \tStep: 688 \tLoss: 81.4057 \n",
            "Epoch: 3/5 \tStep: 689 \tLoss: 78.1664 \n",
            "Epoch: 3/5 \tStep: 690 \tLoss: 77.5737 \n",
            "Epoch: 3/5 \tStep: 691 \tLoss: 80.9132 \n",
            "Epoch: 3/5 \tStep: 692 \tLoss: 80.2749 \n",
            "Epoch: 3/5 \tStep: 693 \tLoss: 97.9832 \n",
            "Epoch: 3/5 \tStep: 694 \tLoss: 80.6025 \n",
            "Epoch: 3/5 \tStep: 695 \tLoss: 77.8595 \n",
            "Epoch: 3/5 \tStep: 696 \tLoss: 81.0350 \n",
            "Epoch: 3/5 \tStep: 697 \tLoss: 76.5286 \n",
            "Epoch: 3/5 \tStep: 698 \tLoss: 88.0082 \n",
            "Epoch: 3/5 \tStep: 699 \tLoss: 80.7518 \n",
            "Epoch: 3/5 \tStep: 700 \tLoss: 89.6362 \n",
            "Epoch: 3/5 \tStep: 701 \tLoss: 82.8870 \n",
            "Epoch: 3/5 \tStep: 702 \tLoss: 76.6946 \n",
            "Epoch: 3/5 \tStep: 703 \tLoss: 81.2808 \n",
            "Epoch: 3/5 \tStep: 704 \tLoss: 77.0105 \n",
            "Epoch: 3/5 \tStep: 705 \tLoss: 81.1879 \n",
            "Epoch: 3/5 \tStep: 706 \tLoss: 81.0081 \n",
            "Epoch: 3/5 \tStep: 707 \tLoss: 80.9223 \n",
            "Epoch: 3/5 \tStep: 708 \tLoss: 77.1566 \n",
            "Epoch: 3/5 \tStep: 709 \tLoss: 77.6836 \n",
            "Epoch: 3/5 \tStep: 710 \tLoss: 80.7160 \n",
            "Epoch: 3/5 \tStep: 711 \tLoss: 80.8662 \n",
            "Epoch: 3/5 \tStep: 712 \tLoss: 80.5088 \n",
            "Epoch: 3/5 \tStep: 713 \tLoss: 81.3344 \n",
            "Epoch: 3/5 \tStep: 714 \tLoss: 80.3237 \n",
            "Epoch: 3/5 \tStep: 715 \tLoss: 80.7437 \n",
            "Epoch: 3/5 \tStep: 716 \tLoss: 80.8483 \n",
            "Epoch: 3/5 \tStep: 717 \tLoss: 81.3923 \n",
            "Epoch: 3/5 \tStep: 718 \tLoss: 77.5014 \n",
            "Epoch: 3/5 \tStep: 719 \tLoss: 80.2497 \n",
            "Epoch: 3/5 \tStep: 720 \tLoss: 81.2035 \n",
            "Epoch: 3/5 \tStep: 721 \tLoss: 81.5214 \n",
            "Epoch: 3/5 \tStep: 722 \tLoss: 81.0862 \n",
            "Epoch: 3/5 \tStep: 723 \tLoss: 81.0054 \n",
            "Epoch: 3/5 \tStep: 724 \tLoss: 78.8783 \n",
            "Epoch: 3/5 \tStep: 725 \tLoss: 80.7520 \n",
            "Epoch: 3/5 \tStep: 726 \tLoss: 80.9456 \n",
            "Epoch: 3/5 \tStep: 727 \tLoss: 80.4163 \n",
            "Epoch: 3/5 \tStep: 728 \tLoss: 77.1261 \n",
            "Epoch: 3/5 \tStep: 729 \tLoss: 82.7622 \n",
            "Epoch: 3/5 \tStep: 730 \tLoss: 78.9380 \n",
            "Epoch: 3/5 \tStep: 731 \tLoss: 81.6134 \n",
            "Epoch: 3/5 \tStep: 732 \tLoss: 83.4519 \n",
            "Epoch: 3/5 \tStep: 733 \tLoss: 93.2732 \n",
            "Epoch: 3/5 \tStep: 734 \tLoss: 77.4681 \n",
            "Epoch: 3/5 \tStep: 735 \tLoss: 81.4398 \n",
            "Epoch: 3/5 \tStep: 736 \tLoss: 81.2720 \n",
            "Epoch: 3/5 \tStep: 737 \tLoss: 80.2060 \n",
            "Epoch: 3/5 \tStep: 738 \tLoss: 82.9402 \n",
            "Epoch: 3/5 \tStep: 739 \tLoss: 77.5447 \n",
            "Epoch: 3/5 \tStep: 740 \tLoss: 81.3774 \n",
            "Epoch: 3/5 \tStep: 741 \tLoss: 76.7027 \n",
            "Epoch: 3/5 \tStep: 742 \tLoss: 81.5738 \n",
            "Epoch: 3/5 \tStep: 743 \tLoss: 80.5476 \n",
            "Epoch: 3/5 \tStep: 744 \tLoss: 77.6577 \n",
            "Epoch: 3/5 \tStep: 745 \tLoss: 87.1949 \n",
            "Epoch: 3/5 \tStep: 746 \tLoss: 77.3784 \n",
            "Epoch: 3/5 \tStep: 747 \tLoss: 77.9910 \n",
            "Epoch: 3/5 \tStep: 748 \tLoss: 80.9256 \n",
            "Epoch: 3/5 \tStep: 749 \tLoss: 77.7439 \n",
            "Epoch: 3/5 \tStep: 750 \tLoss: 80.7510 \n",
            "Epoch: 3/5 \tStep: 751 \tLoss: 82.1798 \n",
            "Epoch: 3/5 \tStep: 752 \tLoss: 76.5737 \n",
            "Epoch: 3/5 \tStep: 753 \tLoss: 79.4971 \n",
            "Epoch: 3/5 \tStep: 754 \tLoss: 81.0591 \n",
            "Epoch: 3/5 \tStep: 755 \tLoss: 80.2198 \n",
            "Epoch: 3/5 \tStep: 756 \tLoss: 76.8099 \n",
            "Epoch: 3/5 \tStep: 757 \tLoss: 77.2676 \n",
            "Epoch: 3/5 \tStep: 758 \tLoss: 78.0863 \n",
            "Epoch: 3/5 \tStep: 759 \tLoss: 82.6725 \n",
            "Epoch: 3/5 \tStep: 760 \tLoss: 90.3805 \n",
            "Epoch: 3/5 \tStep: 761 \tLoss: 84.8215 \n",
            "Epoch: 3/5 \tStep: 762 \tLoss: 80.7005 \n",
            "Epoch: 3/5 \tStep: 763 \tLoss: 81.7312 \n",
            "Epoch: 3/5 \tStep: 764 \tLoss: 80.3419 \n",
            "Epoch: 3/5 \tStep: 765 \tLoss: 80.5200 \n",
            "Epoch: 3/5 \tStep: 766 \tLoss: 79.3074 \n",
            "Epoch: 3/5 \tStep: 767 \tLoss: 78.3648 \n",
            "Epoch: 3/5 \tStep: 768 \tLoss: 80.7587 \n",
            "Epoch: 3/5 \tStep: 769 \tLoss: 76.9638 \n",
            "Epoch: 3/5 \tStep: 770 \tLoss: 88.2967 \n",
            "Epoch: 3/5 \tStep: 771 \tLoss: 81.0682 \n",
            "Epoch: 3/5 \tStep: 772 \tLoss: 77.7283 \n",
            "Epoch: 3/5 \tStep: 773 \tLoss: 83.1837 \n",
            "Epoch: 3/5 \tStep: 774 \tLoss: 80.5949 \n",
            "Epoch: 3/5 \tStep: 775 \tLoss: 86.7391 \n",
            "Epoch: 3/5 \tStep: 776 \tLoss: 80.0522 \n",
            "Epoch: 3/5 \tStep: 777 \tLoss: 76.5066 \n",
            "Epoch: 3/5 \tStep: 778 \tLoss: 76.6897 \n",
            "Epoch: 3/5 \tStep: 779 \tLoss: 88.2670 \n",
            "Epoch: 3/5 \tStep: 780 \tLoss: 81.4198 \n",
            "Epoch: 3/5 \tStep: 781 \tLoss: 80.4751 \n",
            "Epoch: 3/5 \tStep: 782 \tLoss: 80.7708 \n",
            "Epoch: 3/5 \tStep: 783 \tLoss: 77.6506 \n",
            "Epoch: 3/5 \tStep: 784 \tLoss: 93.8605 \n",
            "Epoch: 3/5 \tStep: 785 \tLoss: 80.8473 \n",
            "Epoch: 3/5 \tStep: 786 \tLoss: 76.7805 \n",
            "Epoch: 3/5 \tStep: 787 \tLoss: 80.7126 \n",
            "Epoch: 3/5 \tStep: 788 \tLoss: 80.3718 \n",
            "Epoch: 3/5 \tStep: 789 \tLoss: 81.2357 \n",
            "Epoch: 3/5 \tStep: 790 \tLoss: 82.7222 \n",
            "Epoch: 3/5 \tStep: 791 \tLoss: 81.2438 \n",
            "Epoch: 3/5 \tStep: 792 \tLoss: 81.5912 \n",
            "Epoch: 3/5 \tStep: 793 \tLoss: 88.0876 \n",
            "Epoch: 3/5 \tStep: 794 \tLoss: 78.3286 \n",
            "Epoch: 3/5 \tStep: 795 \tLoss: 83.6684 \n",
            "Epoch: 3/5 \tStep: 796 \tLoss: 77.0995 \n",
            "Epoch: 3/5 \tStep: 797 \tLoss: 81.4423 \n",
            "Epoch: 3/5 \tStep: 798 \tLoss: 88.0030 \n",
            "Epoch: 3/5 \tStep: 799 \tLoss: 82.0335 \n",
            "Epoch: 3/5 \tStep: 800 \tLoss: 80.2245 \n",
            "Epoch: 3/5 \tStep: 801 \tLoss: 81.0962 \n",
            "Epoch: 3/5 \tStep: 802 \tLoss: 76.1929 \n",
            "Epoch: 3/5 \tStep: 803 \tLoss: 80.4625 \n",
            "Epoch: 3/5 \tStep: 804 \tLoss: 77.3968 \n",
            "Epoch: 3/5 \tStep: 805 \tLoss: 81.2142 \n",
            "Epoch: 3/5 \tStep: 806 \tLoss: 83.0093 \n",
            "Epoch: 3/5 \tStep: 807 \tLoss: 82.2703 \n",
            "Epoch: 3/5 \tStep: 808 \tLoss: 81.5271 \n",
            "Epoch: 3/5 \tStep: 809 \tLoss: 80.1895 \n",
            "Epoch: 3/5 \tStep: 810 \tLoss: 80.7528 \n",
            "Epoch: 3/5 \tStep: 811 \tLoss: 80.1936 \n",
            "Epoch: 3/5 \tStep: 812 \tLoss: 82.1984 \n",
            "Epoch: 3/5 \tStep: 813 \tLoss: 81.3450 \n",
            "Epoch: 3/5 \tStep: 814 \tLoss: 78.8038 \n",
            "Epoch: 3/5 \tStep: 815 \tLoss: 76.9926 \n",
            "Epoch: 3/5 \tStep: 816 \tLoss: 92.6719 \n",
            "Epoch: 3/5 \tStep: 817 \tLoss: 80.7666 \n",
            "Epoch: 3/5 \tStep: 818 \tLoss: 80.3089 \n",
            "Epoch: 3/5 \tStep: 819 \tLoss: 77.2356 \n",
            "Epoch: 3/5 \tStep: 820 \tLoss: 79.7421 \n",
            "Epoch: 3/5 \tStep: 821 \tLoss: 80.7263 \n",
            "Epoch: 3/5 \tStep: 822 \tLoss: 80.4555 \n",
            "Epoch: 3/5 \tStep: 823 \tLoss: 80.4544 \n",
            "Epoch: 3/5 \tStep: 824 \tLoss: 77.3592 \n",
            "Epoch: 3/5 \tStep: 825 \tLoss: 80.9307 \n",
            "Epoch: 3/5 \tStep: 826 \tLoss: 86.1210 \n",
            "Epoch: 3/5 \tStep: 827 \tLoss: 77.3356 \n",
            "Epoch: 3/5 \tStep: 828 \tLoss: 80.7592 \n",
            "Epoch: 3/5 \tStep: 829 \tLoss: 77.5898 \n",
            "Epoch: 3/5 \tStep: 830 \tLoss: 80.7231 \n",
            "Epoch: 3/5 \tStep: 831 \tLoss: 77.7644 \n",
            "Epoch: 3/5 \tStep: 832 \tLoss: 77.3610 \n",
            "Epoch: 3/5 \tStep: 833 \tLoss: 80.1437 \n",
            "Epoch: 3/5 \tStep: 834 \tLoss: 79.7984 \n",
            "Epoch: 3/5 \tStep: 835 \tLoss: 93.9306 \n",
            "Epoch: 3/5 \tStep: 836 \tLoss: 80.6325 \n",
            "Epoch: 3/5 \tStep: 837 \tLoss: 82.1405 \n",
            "Epoch: 3/5 \tStep: 838 \tLoss: 78.4480 \n",
            "Epoch: 3/5 \tStep: 839 \tLoss: 80.7310 \n",
            "Epoch: 3/5 \tStep: 840 \tLoss: 82.0796 \n",
            "Epoch: 3/5 \tStep: 841 \tLoss: 77.5350 \n",
            "Epoch: 3/5 \tStep: 842 \tLoss: 76.9566 \n",
            "Epoch: 3/5 \tStep: 843 \tLoss: 76.2610 \n",
            "Epoch: 3/5 \tStep: 844 \tLoss: 80.3508 \n",
            "Epoch: 3/5 \tStep: 845 \tLoss: 79.7126 \n",
            "Epoch: 3/5 \tStep: 846 \tLoss: 87.2668 \n",
            "Epoch: 3/5 \tStep: 847 \tLoss: 79.4055 \n",
            "Epoch: 3/5 \tStep: 848 \tLoss: 82.3393 \n",
            "Epoch: 3/5 \tStep: 849 \tLoss: 82.7262 \n",
            "Epoch: 3/5 \tStep: 850 \tLoss: 81.7364 \n",
            "Epoch: 3/5 \tStep: 851 \tLoss: 82.1049 \n",
            "Epoch: 3/5 \tStep: 852 \tLoss: 77.1915 \n",
            "Epoch: 3/5 \tStep: 853 \tLoss: 80.8413 \n",
            "Epoch: 3/5 \tStep: 854 \tLoss: 77.5263 \n",
            "Epoch: 4/5 \tStep: 855 \tLoss: 78.2870 \n",
            "Epoch: 4/5 \tStep: 856 \tLoss: 77.8329 \n",
            "Epoch: 4/5 \tStep: 857 \tLoss: 93.5909 \n",
            "Epoch: 4/5 \tStep: 858 \tLoss: 80.0275 \n",
            "Epoch: 4/5 \tStep: 859 \tLoss: 77.1261 \n",
            "Epoch: 4/5 \tStep: 860 \tLoss: 86.1596 \n",
            "Epoch: 4/5 \tStep: 861 \tLoss: 81.6383 \n",
            "Epoch: 4/5 \tStep: 862 \tLoss: 81.8751 \n",
            "Epoch: 4/5 \tStep: 863 \tLoss: 82.4292 \n",
            "Epoch: 4/5 \tStep: 864 \tLoss: 81.4774 \n",
            "Epoch: 4/5 \tStep: 865 \tLoss: 80.1110 \n",
            "Epoch: 4/5 \tStep: 866 \tLoss: 77.7781 \n",
            "Epoch: 4/5 \tStep: 867 \tLoss: 77.2235 \n",
            "Epoch: 4/5 \tStep: 868 \tLoss: 81.2340 \n",
            "Epoch: 4/5 \tStep: 869 \tLoss: 81.7158 \n",
            "Epoch: 4/5 \tStep: 870 \tLoss: 82.7793 \n",
            "Epoch: 4/5 \tStep: 871 \tLoss: 77.8401 \n",
            "Epoch: 4/5 \tStep: 872 \tLoss: 80.3939 \n",
            "Epoch: 4/5 \tStep: 873 \tLoss: 80.7363 \n",
            "Epoch: 4/5 \tStep: 874 \tLoss: 88.4832 \n",
            "Epoch: 4/5 \tStep: 875 \tLoss: 76.7853 \n",
            "Epoch: 4/5 \tStep: 876 \tLoss: 80.4634 \n",
            "Epoch: 4/5 \tStep: 877 \tLoss: 80.6046 \n",
            "Epoch: 4/5 \tStep: 878 \tLoss: 80.9655 \n",
            "Epoch: 4/5 \tStep: 879 \tLoss: 77.6574 \n",
            "Epoch: 4/5 \tStep: 880 \tLoss: 86.1239 \n",
            "Epoch: 4/5 \tStep: 881 \tLoss: 80.7942 \n",
            "Epoch: 4/5 \tStep: 882 \tLoss: 90.6554 \n",
            "Epoch: 4/5 \tStep: 883 \tLoss: 80.5423 \n",
            "Epoch: 4/5 \tStep: 884 \tLoss: 81.9082 \n",
            "Epoch: 4/5 \tStep: 885 \tLoss: 81.4447 \n",
            "Epoch: 4/5 \tStep: 886 \tLoss: 81.8187 \n",
            "Epoch: 4/5 \tStep: 887 \tLoss: 81.9345 \n",
            "Epoch: 4/5 \tStep: 888 \tLoss: 77.8032 \n",
            "Epoch: 4/5 \tStep: 889 \tLoss: 83.4135 \n",
            "Epoch: 4/5 \tStep: 890 \tLoss: 90.4088 \n",
            "Epoch: 4/5 \tStep: 891 \tLoss: 81.6456 \n",
            "Epoch: 4/5 \tStep: 892 \tLoss: 81.0333 \n",
            "Epoch: 4/5 \tStep: 893 \tLoss: 80.4115 \n",
            "Epoch: 4/5 \tStep: 894 \tLoss: 77.2541 \n",
            "Epoch: 4/5 \tStep: 895 \tLoss: 80.6030 \n",
            "Epoch: 4/5 \tStep: 896 \tLoss: 86.1887 \n",
            "Epoch: 4/5 \tStep: 897 \tLoss: 76.9161 \n",
            "Epoch: 4/5 \tStep: 898 \tLoss: 77.0596 \n",
            "Epoch: 4/5 \tStep: 899 \tLoss: 81.6700 \n",
            "Epoch: 4/5 \tStep: 900 \tLoss: 76.7619 \n",
            "Epoch: 4/5 \tStep: 901 \tLoss: 83.1489 \n",
            "Epoch: 4/5 \tStep: 902 \tLoss: 77.1980 \n",
            "Epoch: 4/5 \tStep: 903 \tLoss: 76.0341 \n",
            "Epoch: 4/5 \tStep: 904 \tLoss: 78.0474 \n",
            "Epoch: 4/5 \tStep: 905 \tLoss: 76.6095 \n",
            "Epoch: 4/5 \tStep: 906 \tLoss: 81.1355 \n",
            "Epoch: 4/5 \tStep: 907 \tLoss: 81.2709 \n",
            "Epoch: 4/5 \tStep: 908 \tLoss: 81.1102 \n",
            "Epoch: 4/5 \tStep: 909 \tLoss: 88.0349 \n",
            "Epoch: 4/5 \tStep: 910 \tLoss: 82.1090 \n",
            "Epoch: 4/5 \tStep: 911 \tLoss: 77.8122 \n",
            "Epoch: 4/5 \tStep: 912 \tLoss: 76.8673 \n",
            "Epoch: 4/5 \tStep: 913 \tLoss: 94.5934 \n",
            "Epoch: 4/5 \tStep: 914 \tLoss: 78.1693 \n",
            "Epoch: 4/5 \tStep: 915 \tLoss: 75.7193 \n",
            "Epoch: 4/5 \tStep: 916 \tLoss: 76.1178 \n",
            "Epoch: 4/5 \tStep: 917 \tLoss: 76.5744 \n",
            "Epoch: 4/5 \tStep: 918 \tLoss: 82.8683 \n",
            "Epoch: 4/5 \tStep: 919 \tLoss: 77.2496 \n",
            "Epoch: 4/5 \tStep: 920 \tLoss: 81.5560 \n",
            "Epoch: 4/5 \tStep: 921 \tLoss: 80.7917 \n",
            "Epoch: 4/5 \tStep: 922 \tLoss: 81.6288 \n",
            "Epoch: 4/5 \tStep: 923 \tLoss: 80.5035 \n",
            "Epoch: 4/5 \tStep: 924 \tLoss: 77.1956 \n",
            "Epoch: 4/5 \tStep: 925 \tLoss: 81.5336 \n",
            "Epoch: 4/5 \tStep: 926 \tLoss: 77.4628 \n",
            "Epoch: 4/5 \tStep: 927 \tLoss: 76.4491 \n",
            "Epoch: 4/5 \tStep: 928 \tLoss: 79.5420 \n",
            "Epoch: 4/5 \tStep: 929 \tLoss: 79.8271 \n",
            "Epoch: 4/5 \tStep: 930 \tLoss: 79.9377 \n",
            "Epoch: 4/5 \tStep: 931 \tLoss: 81.2420 \n",
            "Epoch: 4/5 \tStep: 932 \tLoss: 81.3252 \n",
            "Epoch: 4/5 \tStep: 933 \tLoss: 78.5800 \n",
            "Epoch: 4/5 \tStep: 934 \tLoss: 81.0498 \n",
            "Epoch: 4/5 \tStep: 935 \tLoss: 80.7278 \n",
            "Epoch: 4/5 \tStep: 936 \tLoss: 78.3253 \n",
            "Epoch: 4/5 \tStep: 937 \tLoss: 77.2992 \n",
            "Epoch: 4/5 \tStep: 938 \tLoss: 80.5117 \n",
            "Epoch: 4/5 \tStep: 939 \tLoss: 79.5884 \n",
            "Epoch: 4/5 \tStep: 940 \tLoss: 79.5431 \n",
            "Epoch: 4/5 \tStep: 941 \tLoss: 79.0215 \n",
            "Epoch: 4/5 \tStep: 942 \tLoss: 80.8381 \n",
            "Epoch: 4/5 \tStep: 943 \tLoss: 85.4779 \n",
            "Epoch: 4/5 \tStep: 944 \tLoss: 76.4417 \n",
            "Epoch: 4/5 \tStep: 945 \tLoss: 80.9031 \n",
            "Epoch: 4/5 \tStep: 946 \tLoss: 82.7539 \n",
            "Epoch: 4/5 \tStep: 947 \tLoss: 86.7166 \n",
            "Epoch: 4/5 \tStep: 948 \tLoss: 83.0046 \n",
            "Epoch: 4/5 \tStep: 949 \tLoss: 80.7577 \n",
            "Epoch: 4/5 \tStep: 950 \tLoss: 77.0864 \n",
            "Epoch: 4/5 \tStep: 951 \tLoss: 80.8102 \n",
            "Epoch: 4/5 \tStep: 952 \tLoss: 81.4824 \n",
            "Epoch: 4/5 \tStep: 953 \tLoss: 80.8648 \n",
            "Epoch: 4/5 \tStep: 954 \tLoss: 80.7806 \n",
            "Epoch: 4/5 \tStep: 955 \tLoss: 81.8589 \n",
            "Epoch: 4/5 \tStep: 956 \tLoss: 86.4707 \n",
            "Epoch: 4/5 \tStep: 957 \tLoss: 79.4236 \n",
            "Epoch: 4/5 \tStep: 958 \tLoss: 77.0011 \n",
            "Epoch: 4/5 \tStep: 959 \tLoss: 80.1581 \n",
            "Epoch: 4/5 \tStep: 960 \tLoss: 76.4856 \n",
            "Epoch: 4/5 \tStep: 961 \tLoss: 80.4530 \n",
            "Epoch: 4/5 \tStep: 962 \tLoss: 80.0902 \n",
            "Epoch: 4/5 \tStep: 963 \tLoss: 80.3427 \n",
            "Epoch: 4/5 \tStep: 964 \tLoss: 80.4620 \n",
            "Epoch: 4/5 \tStep: 965 \tLoss: 81.6630 \n",
            "Epoch: 4/5 \tStep: 966 \tLoss: 77.5098 \n",
            "Epoch: 4/5 \tStep: 967 \tLoss: 76.0856 \n",
            "Epoch: 4/5 \tStep: 968 \tLoss: 75.6903 \n",
            "Epoch: 4/5 \tStep: 969 \tLoss: 77.3643 \n",
            "Epoch: 4/5 \tStep: 970 \tLoss: 78.9297 \n",
            "Epoch: 4/5 \tStep: 971 \tLoss: 75.8139 \n",
            "Epoch: 4/5 \tStep: 972 \tLoss: 80.0107 \n",
            "Epoch: 4/5 \tStep: 973 \tLoss: 75.4391 \n",
            "Epoch: 4/5 \tStep: 974 \tLoss: 82.5861 \n",
            "Epoch: 4/5 \tStep: 975 \tLoss: 80.8100 \n",
            "Epoch: 4/5 \tStep: 976 \tLoss: 77.5488 \n",
            "Epoch: 4/5 \tStep: 977 \tLoss: 82.8354 \n",
            "Epoch: 4/5 \tStep: 978 \tLoss: 76.6873 \n",
            "Epoch: 4/5 \tStep: 979 \tLoss: 80.2512 \n",
            "Epoch: 4/5 \tStep: 980 \tLoss: 80.5203 \n",
            "Epoch: 4/5 \tStep: 981 \tLoss: 85.4688 \n",
            "Epoch: 4/5 \tStep: 982 \tLoss: 79.9083 \n",
            "Epoch: 4/5 \tStep: 983 \tLoss: 79.3717 \n",
            "Epoch: 4/5 \tStep: 984 \tLoss: 76.2060 \n",
            "Epoch: 4/5 \tStep: 985 \tLoss: 80.6810 \n",
            "Epoch: 4/5 \tStep: 986 \tLoss: 80.1013 \n",
            "Epoch: 4/5 \tStep: 987 \tLoss: 82.1859 \n",
            "Epoch: 4/5 \tStep: 988 \tLoss: 86.9381 \n",
            "Epoch: 4/5 \tStep: 989 \tLoss: 81.1960 \n",
            "Epoch: 4/5 \tStep: 990 \tLoss: 80.4181 \n",
            "Epoch: 4/5 \tStep: 991 \tLoss: 80.9839 \n",
            "Epoch: 4/5 \tStep: 992 \tLoss: 80.7361 \n",
            "Epoch: 4/5 \tStep: 993 \tLoss: 79.0398 \n",
            "Epoch: 4/5 \tStep: 994 \tLoss: 81.0920 \n",
            "Epoch: 4/5 \tStep: 995 \tLoss: 90.7454 \n",
            "Epoch: 4/5 \tStep: 996 \tLoss: 93.1121 \n",
            "Epoch: 4/5 \tStep: 997 \tLoss: 81.0773 \n",
            "Epoch: 4/5 \tStep: 998 \tLoss: 77.3975 \n",
            "Epoch: 4/5 \tStep: 999 \tLoss: 82.5157 \n",
            "Epoch: 4/5 \tStep: 1000 \tLoss: 82.2947 \n",
            "Epoch: 4/5 \tStep: 1001 \tLoss: 76.3274 \n",
            "Epoch: 4/5 \tStep: 1002 \tLoss: 76.7904 \n",
            "Epoch: 4/5 \tStep: 1003 \tLoss: 77.9185 \n",
            "Epoch: 4/5 \tStep: 1004 \tLoss: 75.8939 \n",
            "Epoch: 4/5 \tStep: 1005 \tLoss: 87.2398 \n",
            "Epoch: 4/5 \tStep: 1006 \tLoss: 80.3725 \n",
            "Epoch: 4/5 \tStep: 1007 \tLoss: 80.5533 \n",
            "Epoch: 4/5 \tStep: 1008 \tLoss: 80.0311 \n",
            "Epoch: 4/5 \tStep: 1009 \tLoss: 81.9886 \n",
            "Epoch: 4/5 \tStep: 1010 \tLoss: 80.8865 \n",
            "Epoch: 4/5 \tStep: 1011 \tLoss: 81.8053 \n",
            "Epoch: 4/5 \tStep: 1012 \tLoss: 92.1728 \n",
            "Epoch: 4/5 \tStep: 1013 \tLoss: 81.7700 \n",
            "Epoch: 4/5 \tStep: 1014 \tLoss: 81.2682 \n",
            "Epoch: 4/5 \tStep: 1015 \tLoss: 81.6023 \n",
            "Epoch: 4/5 \tStep: 1016 \tLoss: 80.6171 \n",
            "Epoch: 4/5 \tStep: 1017 \tLoss: 80.5414 \n",
            "Epoch: 4/5 \tStep: 1018 \tLoss: 76.8888 \n",
            "Epoch: 4/5 \tStep: 1019 \tLoss: 77.7325 \n",
            "Epoch: 4/5 \tStep: 1020 \tLoss: 80.7629 \n",
            "Epoch: 4/5 \tStep: 1021 \tLoss: 76.8975 \n",
            "Epoch: 4/5 \tStep: 1022 \tLoss: 80.1906 \n",
            "Epoch: 4/5 \tStep: 1023 \tLoss: 78.2016 \n",
            "Epoch: 4/5 \tStep: 1024 \tLoss: 86.1483 \n",
            "Epoch: 4/5 \tStep: 1025 \tLoss: 80.2683 \n",
            "Epoch: 4/5 \tStep: 1026 \tLoss: 81.9112 \n",
            "Epoch: 4/5 \tStep: 1027 \tLoss: 76.0827 \n",
            "Epoch: 4/5 \tStep: 1028 \tLoss: 86.3750 \n",
            "Epoch: 4/5 \tStep: 1029 \tLoss: 77.5526 \n",
            "Epoch: 4/5 \tStep: 1030 \tLoss: 86.7367 \n",
            "Epoch: 4/5 \tStep: 1031 \tLoss: 81.2932 \n",
            "Epoch: 4/5 \tStep: 1032 \tLoss: 81.3114 \n",
            "Epoch: 4/5 \tStep: 1033 \tLoss: 76.8260 \n",
            "Epoch: 4/5 \tStep: 1034 \tLoss: 80.1063 \n",
            "Epoch: 4/5 \tStep: 1035 \tLoss: 77.8753 \n",
            "Epoch: 4/5 \tStep: 1036 \tLoss: 76.7378 \n",
            "Epoch: 4/5 \tStep: 1037 \tLoss: 76.7460 \n",
            "Epoch: 4/5 \tStep: 1038 \tLoss: 80.9885 \n",
            "Epoch: 4/5 \tStep: 1039 \tLoss: 81.4611 \n",
            "Epoch: 4/5 \tStep: 1040 \tLoss: 77.4838 \n",
            "Epoch: 4/5 \tStep: 1041 \tLoss: 77.4016 \n",
            "Epoch: 4/5 \tStep: 1042 \tLoss: 95.3786 \n",
            "Epoch: 4/5 \tStep: 1043 \tLoss: 80.8357 \n",
            "Epoch: 4/5 \tStep: 1044 \tLoss: 80.4927 \n",
            "Epoch: 4/5 \tStep: 1045 \tLoss: 79.5092 \n",
            "Epoch: 4/5 \tStep: 1046 \tLoss: 76.5456 \n",
            "Epoch: 4/5 \tStep: 1047 \tLoss: 81.3197 \n",
            "Epoch: 4/5 \tStep: 1048 \tLoss: 80.3071 \n",
            "Epoch: 4/5 \tStep: 1049 \tLoss: 80.3682 \n",
            "Epoch: 4/5 \tStep: 1050 \tLoss: 80.0950 \n",
            "Epoch: 4/5 \tStep: 1051 \tLoss: 86.3941 \n",
            "Epoch: 4/5 \tStep: 1052 \tLoss: 80.6244 \n",
            "Epoch: 4/5 \tStep: 1053 \tLoss: 80.5284 \n",
            "Epoch: 4/5 \tStep: 1054 \tLoss: 76.9456 \n",
            "Epoch: 4/5 \tStep: 1055 \tLoss: 77.4010 \n",
            "Epoch: 4/5 \tStep: 1056 \tLoss: 80.1424 \n",
            "Epoch: 4/5 \tStep: 1057 \tLoss: 79.8598 \n",
            "Epoch: 4/5 \tStep: 1058 \tLoss: 79.6826 \n",
            "Epoch: 4/5 \tStep: 1059 \tLoss: 86.5002 \n",
            "Epoch: 4/5 \tStep: 1060 \tLoss: 76.8644 \n",
            "Epoch: 4/5 \tStep: 1061 \tLoss: 81.5505 \n",
            "Epoch: 4/5 \tStep: 1062 \tLoss: 81.0179 \n",
            "Epoch: 4/5 \tStep: 1063 \tLoss: 82.2553 \n",
            "Epoch: 4/5 \tStep: 1064 \tLoss: 76.8854 \n",
            "Epoch: 4/5 \tStep: 1065 \tLoss: 83.8115 \n",
            "Epoch: 4/5 \tStep: 1066 \tLoss: 79.3782 \n",
            "Epoch: 4/5 \tStep: 1067 \tLoss: 78.1082 \n",
            "Epoch: 4/5 \tStep: 1068 \tLoss: 78.2228 \n",
            "Epoch: 4/5 \tStep: 1069 \tLoss: 80.4573 \n",
            "Epoch: 4/5 \tStep: 1070 \tLoss: 76.6927 \n",
            "Epoch: 4/5 \tStep: 1071 \tLoss: 79.8152 \n",
            "Epoch: 4/5 \tStep: 1072 \tLoss: 77.6107 \n",
            "Epoch: 4/5 \tStep: 1073 \tLoss: 87.5551 \n",
            "Epoch: 4/5 \tStep: 1074 \tLoss: 80.2513 \n",
            "Epoch: 4/5 \tStep: 1075 \tLoss: 76.8872 \n",
            "Epoch: 4/5 \tStep: 1076 \tLoss: 81.1149 \n",
            "Epoch: 4/5 \tStep: 1077 \tLoss: 80.5167 \n",
            "Epoch: 4/5 \tStep: 1078 \tLoss: 82.8749 \n",
            "Epoch: 4/5 \tStep: 1079 \tLoss: 80.0167 \n",
            "Epoch: 4/5 \tStep: 1080 \tLoss: 76.3739 \n",
            "Epoch: 4/5 \tStep: 1081 \tLoss: 80.2095 \n",
            "Epoch: 4/5 \tStep: 1082 \tLoss: 78.0189 \n",
            "Epoch: 4/5 \tStep: 1083 \tLoss: 77.1116 \n",
            "Epoch: 4/5 \tStep: 1084 \tLoss: 80.8235 \n",
            "Epoch: 4/5 \tStep: 1085 \tLoss: 78.5872 \n",
            "Epoch: 4/5 \tStep: 1086 \tLoss: 77.7802 \n",
            "Epoch: 4/5 \tStep: 1087 \tLoss: 77.1710 \n",
            "Epoch: 4/5 \tStep: 1088 \tLoss: 81.8173 \n",
            "Epoch: 4/5 \tStep: 1089 \tLoss: 77.4356 \n",
            "Epoch: 4/5 \tStep: 1090 \tLoss: 80.1850 \n",
            "Epoch: 4/5 \tStep: 1091 \tLoss: 76.8657 \n",
            "Epoch: 4/5 \tStep: 1092 \tLoss: 80.2371 \n",
            "Epoch: 4/5 \tStep: 1093 \tLoss: 76.9479 \n",
            "Epoch: 4/5 \tStep: 1094 \tLoss: 81.3722 \n",
            "Epoch: 4/5 \tStep: 1095 \tLoss: 80.7576 \n",
            "Epoch: 4/5 \tStep: 1096 \tLoss: 82.4625 \n",
            "Epoch: 4/5 \tStep: 1097 \tLoss: 85.3605 \n",
            "Epoch: 4/5 \tStep: 1098 \tLoss: 80.5523 \n",
            "Epoch: 4/5 \tStep: 1099 \tLoss: 88.4526 \n",
            "Epoch: 4/5 \tStep: 1100 \tLoss: 80.2262 \n",
            "Epoch: 4/5 \tStep: 1101 \tLoss: 80.1634 \n",
            "Epoch: 4/5 \tStep: 1102 \tLoss: 80.7831 \n",
            "Epoch: 4/5 \tStep: 1103 \tLoss: 79.0471 \n",
            "Epoch: 4/5 \tStep: 1104 \tLoss: 80.0206 \n",
            "Epoch: 4/5 \tStep: 1105 \tLoss: 91.9058 \n",
            "Epoch: 4/5 \tStep: 1106 \tLoss: 80.7617 \n",
            "Epoch: 4/5 \tStep: 1107 \tLoss: 79.6555 \n",
            "Epoch: 4/5 \tStep: 1108 \tLoss: 77.9290 \n",
            "Epoch: 4/5 \tStep: 1109 \tLoss: 77.1945 \n",
            "Epoch: 4/5 \tStep: 1110 \tLoss: 82.5649 \n",
            "Epoch: 4/5 \tStep: 1111 \tLoss: 76.5534 \n",
            "Epoch: 4/5 \tStep: 1112 \tLoss: 82.1024 \n",
            "Epoch: 4/5 \tStep: 1113 \tLoss: 80.6547 \n",
            "Epoch: 4/5 \tStep: 1114 \tLoss: 78.5316 \n",
            "Epoch: 4/5 \tStep: 1115 \tLoss: 80.3331 \n",
            "Epoch: 4/5 \tStep: 1116 \tLoss: 85.9502 \n",
            "Epoch: 4/5 \tStep: 1117 \tLoss: 81.2389 \n",
            "Epoch: 4/5 \tStep: 1118 \tLoss: 90.2366 \n",
            "Epoch: 4/5 \tStep: 1119 \tLoss: 76.4661 \n",
            "Epoch: 4/5 \tStep: 1120 \tLoss: 76.9887 \n",
            "Epoch: 4/5 \tStep: 1121 \tLoss: 82.0171 \n",
            "Epoch: 4/5 \tStep: 1122 \tLoss: 80.3988 \n",
            "Epoch: 4/5 \tStep: 1123 \tLoss: 79.8853 \n",
            "Epoch: 4/5 \tStep: 1124 \tLoss: 77.1506 \n",
            "Epoch: 4/5 \tStep: 1125 \tLoss: 81.4947 \n",
            "Epoch: 4/5 \tStep: 1126 \tLoss: 77.8617 \n",
            "Epoch: 4/5 \tStep: 1127 \tLoss: 86.5493 \n",
            "Epoch: 4/5 \tStep: 1128 \tLoss: 78.1909 \n",
            "Epoch: 4/5 \tStep: 1129 \tLoss: 79.8210 \n",
            "Epoch: 4/5 \tStep: 1130 \tLoss: 72.8333 \n",
            "Epoch: 4/5 \tStep: 1131 \tLoss: 80.5995 \n",
            "Epoch: 4/5 \tStep: 1132 \tLoss: 77.0985 \n",
            "Epoch: 4/5 \tStep: 1133 \tLoss: 82.0306 \n",
            "Epoch: 4/5 \tStep: 1134 \tLoss: 79.7648 \n",
            "Epoch: 4/5 \tStep: 1135 \tLoss: 81.4859 \n",
            "Epoch: 4/5 \tStep: 1136 \tLoss: 89.2848 \n",
            "Epoch: 4/5 \tStep: 1137 \tLoss: 81.5126 \n",
            "Epoch: 4/5 \tStep: 1138 \tLoss: 80.8552 \n",
            "Epoch: 4/5 \tStep: 1139 \tLoss: 80.4936 \n",
            "Epoch: 5/5 \tStep: 1140 \tLoss: 80.2279 \n",
            "Epoch: 5/5 \tStep: 1141 \tLoss: 80.7676 \n",
            "Epoch: 5/5 \tStep: 1142 \tLoss: 81.4996 \n",
            "Epoch: 5/5 \tStep: 1143 \tLoss: 81.7829 \n",
            "Epoch: 5/5 \tStep: 1144 \tLoss: 82.4024 \n",
            "Epoch: 5/5 \tStep: 1145 \tLoss: 80.6414 \n",
            "Epoch: 5/5 \tStep: 1146 \tLoss: 81.0464 \n",
            "Epoch: 5/5 \tStep: 1147 \tLoss: 77.0932 \n",
            "Epoch: 5/5 \tStep: 1148 \tLoss: 75.9006 \n",
            "Epoch: 5/5 \tStep: 1149 \tLoss: 80.0721 \n",
            "Epoch: 5/5 \tStep: 1150 \tLoss: 80.8744 \n",
            "Epoch: 5/5 \tStep: 1151 \tLoss: 85.9227 \n",
            "Epoch: 5/5 \tStep: 1152 \tLoss: 85.3216 \n",
            "Epoch: 5/5 \tStep: 1153 \tLoss: 80.2859 \n",
            "Epoch: 5/5 \tStep: 1154 \tLoss: 100.1735 \n",
            "Epoch: 5/5 \tStep: 1155 \tLoss: 82.1097 \n",
            "Epoch: 5/5 \tStep: 1156 \tLoss: 80.9319 \n",
            "Epoch: 5/5 \tStep: 1157 \tLoss: 77.3004 \n",
            "Epoch: 5/5 \tStep: 1158 \tLoss: 80.9691 \n",
            "Epoch: 5/5 \tStep: 1159 \tLoss: 81.0059 \n",
            "Epoch: 5/5 \tStep: 1160 \tLoss: 76.5633 \n",
            "Epoch: 5/5 \tStep: 1161 \tLoss: 76.0726 \n",
            "Epoch: 5/5 \tStep: 1162 \tLoss: 76.5842 \n",
            "Epoch: 5/5 \tStep: 1163 \tLoss: 77.2231 \n",
            "Epoch: 5/5 \tStep: 1164 \tLoss: 79.3066 \n",
            "Epoch: 5/5 \tStep: 1165 \tLoss: 93.1069 \n",
            "Epoch: 5/5 \tStep: 1166 \tLoss: 81.9357 \n",
            "Epoch: 5/5 \tStep: 1167 \tLoss: 80.1725 \n",
            "Epoch: 5/5 \tStep: 1168 \tLoss: 81.3411 \n",
            "Epoch: 5/5 \tStep: 1169 \tLoss: 76.9078 \n",
            "Epoch: 5/5 \tStep: 1170 \tLoss: 93.5971 \n",
            "Epoch: 5/5 \tStep: 1171 \tLoss: 79.3948 \n",
            "Epoch: 5/5 \tStep: 1172 \tLoss: 79.7495 \n",
            "Epoch: 5/5 \tStep: 1173 \tLoss: 80.9731 \n",
            "Epoch: 5/5 \tStep: 1174 \tLoss: 76.3298 \n",
            "Epoch: 5/5 \tStep: 1175 \tLoss: 88.8367 \n",
            "Epoch: 5/5 \tStep: 1176 \tLoss: 81.2745 \n",
            "Epoch: 5/5 \tStep: 1177 \tLoss: 76.7450 \n",
            "Epoch: 5/5 \tStep: 1178 \tLoss: 84.8057 \n",
            "Epoch: 5/5 \tStep: 1179 \tLoss: 79.3840 \n",
            "Epoch: 5/5 \tStep: 1180 \tLoss: 80.1119 \n",
            "Epoch: 5/5 \tStep: 1181 \tLoss: 76.5122 \n",
            "Epoch: 5/5 \tStep: 1182 \tLoss: 79.5667 \n",
            "Epoch: 5/5 \tStep: 1183 \tLoss: 81.4808 \n",
            "Epoch: 5/5 \tStep: 1184 \tLoss: 77.2802 \n",
            "Epoch: 5/5 \tStep: 1185 \tLoss: 80.6050 \n",
            "Epoch: 5/5 \tStep: 1186 \tLoss: 79.0513 \n",
            "Epoch: 5/5 \tStep: 1187 \tLoss: 76.3109 \n",
            "Epoch: 5/5 \tStep: 1188 \tLoss: 77.0687 \n",
            "Epoch: 5/5 \tStep: 1189 \tLoss: 76.1420 \n",
            "Epoch: 5/5 \tStep: 1190 \tLoss: 81.6668 \n",
            "Epoch: 5/5 \tStep: 1191 \tLoss: 82.2595 \n",
            "Epoch: 5/5 \tStep: 1192 \tLoss: 79.7949 \n",
            "Epoch: 5/5 \tStep: 1193 \tLoss: 80.9422 \n",
            "Epoch: 5/5 \tStep: 1194 \tLoss: 79.6304 \n",
            "Epoch: 5/5 \tStep: 1195 \tLoss: 79.6122 \n",
            "Epoch: 5/5 \tStep: 1196 \tLoss: 76.9629 \n",
            "Epoch: 5/5 \tStep: 1197 \tLoss: 93.4203 \n",
            "Epoch: 5/5 \tStep: 1198 \tLoss: 81.1599 \n",
            "Epoch: 5/5 \tStep: 1199 \tLoss: 80.1336 \n",
            "Epoch: 5/5 \tStep: 1200 \tLoss: 81.7933 \n",
            "Epoch: 5/5 \tStep: 1201 \tLoss: 79.8611 \n",
            "Epoch: 5/5 \tStep: 1202 \tLoss: 82.2641 \n",
            "Epoch: 5/5 \tStep: 1203 \tLoss: 80.5690 \n",
            "Epoch: 5/5 \tStep: 1204 \tLoss: 78.5603 \n",
            "Epoch: 5/5 \tStep: 1205 \tLoss: 79.2768 \n",
            "Epoch: 5/5 \tStep: 1206 \tLoss: 76.2392 \n",
            "Epoch: 5/5 \tStep: 1207 \tLoss: 81.7029 \n",
            "Epoch: 5/5 \tStep: 1208 \tLoss: 79.9281 \n",
            "Epoch: 5/5 \tStep: 1209 \tLoss: 81.3839 \n",
            "Epoch: 5/5 \tStep: 1210 \tLoss: 77.1750 \n",
            "Epoch: 5/5 \tStep: 1211 \tLoss: 80.3152 \n",
            "Epoch: 5/5 \tStep: 1212 \tLoss: 75.8489 \n",
            "Epoch: 5/5 \tStep: 1213 \tLoss: 77.2760 \n",
            "Epoch: 5/5 \tStep: 1214 \tLoss: 76.9572 \n",
            "Epoch: 5/5 \tStep: 1215 \tLoss: 81.6312 \n",
            "Epoch: 5/5 \tStep: 1216 \tLoss: 76.0380 \n",
            "Epoch: 5/5 \tStep: 1217 \tLoss: 79.6216 \n",
            "Epoch: 5/5 \tStep: 1218 \tLoss: 77.1415 \n",
            "Epoch: 5/5 \tStep: 1219 \tLoss: 76.0418 \n",
            "Epoch: 5/5 \tStep: 1220 \tLoss: 76.8230 \n",
            "Epoch: 5/5 \tStep: 1221 \tLoss: 77.2816 \n",
            "Epoch: 5/5 \tStep: 1222 \tLoss: 76.6327 \n",
            "Epoch: 5/5 \tStep: 1223 \tLoss: 81.5833 \n",
            "Epoch: 5/5 \tStep: 1224 \tLoss: 80.4368 \n",
            "Epoch: 5/5 \tStep: 1225 \tLoss: 76.3571 \n",
            "Epoch: 5/5 \tStep: 1226 \tLoss: 82.8783 \n",
            "Epoch: 5/5 \tStep: 1227 \tLoss: 76.6883 \n",
            "Epoch: 5/5 \tStep: 1228 \tLoss: 79.7962 \n",
            "Epoch: 5/5 \tStep: 1229 \tLoss: 92.6144 \n",
            "Epoch: 5/5 \tStep: 1230 \tLoss: 75.5854 \n",
            "Epoch: 5/5 \tStep: 1231 \tLoss: 79.8720 \n",
            "Epoch: 5/5 \tStep: 1232 \tLoss: 80.2575 \n",
            "Epoch: 5/5 \tStep: 1233 \tLoss: 80.5996 \n",
            "Epoch: 5/5 \tStep: 1234 \tLoss: 76.1117 \n",
            "Epoch: 5/5 \tStep: 1235 \tLoss: 76.2758 \n",
            "Epoch: 5/5 \tStep: 1236 \tLoss: 81.4382 \n",
            "Epoch: 5/5 \tStep: 1237 \tLoss: 77.3067 \n",
            "Epoch: 5/5 \tStep: 1238 \tLoss: 80.9077 \n",
            "Epoch: 5/5 \tStep: 1239 \tLoss: 76.9007 \n",
            "Epoch: 5/5 \tStep: 1240 \tLoss: 76.0767 \n",
            "Epoch: 5/5 \tStep: 1241 \tLoss: 76.8499 \n",
            "Epoch: 5/5 \tStep: 1242 \tLoss: 76.8844 \n",
            "Epoch: 5/5 \tStep: 1243 \tLoss: 77.2045 \n",
            "Epoch: 5/5 \tStep: 1244 \tLoss: 77.3804 \n",
            "Epoch: 5/5 \tStep: 1245 \tLoss: 88.2064 \n",
            "Epoch: 5/5 \tStep: 1246 \tLoss: 76.2123 \n",
            "Epoch: 5/5 \tStep: 1247 \tLoss: 81.2719 \n",
            "Epoch: 5/5 \tStep: 1248 \tLoss: 76.9018 \n",
            "Epoch: 5/5 \tStep: 1249 \tLoss: 75.9748 \n",
            "Epoch: 5/5 \tStep: 1250 \tLoss: 78.2797 \n",
            "Epoch: 5/5 \tStep: 1251 \tLoss: 80.9122 \n",
            "Epoch: 5/5 \tStep: 1252 \tLoss: 79.9529 \n",
            "Epoch: 5/5 \tStep: 1253 \tLoss: 76.3589 \n",
            "Epoch: 5/5 \tStep: 1254 \tLoss: 75.7031 \n",
            "Epoch: 5/5 \tStep: 1255 \tLoss: 80.1491 \n",
            "Epoch: 5/5 \tStep: 1256 \tLoss: 81.0359 \n",
            "Epoch: 5/5 \tStep: 1257 \tLoss: 76.9814 \n",
            "Epoch: 5/5 \tStep: 1258 \tLoss: 77.0282 \n",
            "Epoch: 5/5 \tStep: 1259 \tLoss: 80.8415 \n",
            "Epoch: 5/5 \tStep: 1260 \tLoss: 77.5613 \n",
            "Epoch: 5/5 \tStep: 1261 \tLoss: 80.5885 \n",
            "Epoch: 5/5 \tStep: 1262 \tLoss: 89.7828 \n",
            "Epoch: 5/5 \tStep: 1263 \tLoss: 79.4373 \n",
            "Epoch: 5/5 \tStep: 1264 \tLoss: 79.7018 \n",
            "Epoch: 5/5 \tStep: 1265 \tLoss: 86.9564 \n",
            "Epoch: 5/5 \tStep: 1266 \tLoss: 81.0747 \n",
            "Epoch: 5/5 \tStep: 1267 \tLoss: 82.0182 \n",
            "Epoch: 5/5 \tStep: 1268 \tLoss: 77.2408 \n",
            "Epoch: 5/5 \tStep: 1269 \tLoss: 78.0796 \n",
            "Epoch: 5/5 \tStep: 1270 \tLoss: 76.1024 \n",
            "Epoch: 5/5 \tStep: 1271 \tLoss: 81.3847 \n",
            "Epoch: 5/5 \tStep: 1272 \tLoss: 80.4647 \n",
            "Epoch: 5/5 \tStep: 1273 \tLoss: 75.3790 \n",
            "Epoch: 5/5 \tStep: 1274 \tLoss: 79.0774 \n",
            "Epoch: 5/5 \tStep: 1275 \tLoss: 80.1232 \n",
            "Epoch: 5/5 \tStep: 1276 \tLoss: 80.6076 \n",
            "Epoch: 5/5 \tStep: 1277 \tLoss: 80.2896 \n",
            "Epoch: 5/5 \tStep: 1278 \tLoss: 80.8961 \n",
            "Epoch: 5/5 \tStep: 1279 \tLoss: 81.1489 \n",
            "Epoch: 5/5 \tStep: 1280 \tLoss: 80.9103 \n",
            "Epoch: 5/5 \tStep: 1281 \tLoss: 76.6063 \n",
            "Epoch: 5/5 \tStep: 1282 \tLoss: 80.5287 \n",
            "Epoch: 5/5 \tStep: 1283 \tLoss: 79.2501 \n",
            "Epoch: 5/5 \tStep: 1284 \tLoss: 81.4165 \n",
            "Epoch: 5/5 \tStep: 1285 \tLoss: 76.8683 \n",
            "Epoch: 5/5 \tStep: 1286 \tLoss: 75.7371 \n",
            "Epoch: 5/5 \tStep: 1287 \tLoss: 80.3878 \n",
            "Epoch: 5/5 \tStep: 1288 \tLoss: 80.6156 \n",
            "Epoch: 5/5 \tStep: 1289 \tLoss: 81.3258 \n",
            "Epoch: 5/5 \tStep: 1290 \tLoss: 80.3855 \n",
            "Epoch: 5/5 \tStep: 1291 \tLoss: 80.5034 \n",
            "Epoch: 5/5 \tStep: 1292 \tLoss: 81.2430 \n",
            "Epoch: 5/5 \tStep: 1293 \tLoss: 81.0126 \n",
            "Epoch: 5/5 \tStep: 1294 \tLoss: 81.8309 \n",
            "Epoch: 5/5 \tStep: 1295 \tLoss: 79.0242 \n",
            "Epoch: 5/5 \tStep: 1296 \tLoss: 89.1446 \n",
            "Epoch: 5/5 \tStep: 1297 \tLoss: 80.8664 \n",
            "Epoch: 5/5 \tStep: 1298 \tLoss: 87.7801 \n",
            "Epoch: 5/5 \tStep: 1299 \tLoss: 79.8641 \n",
            "Epoch: 5/5 \tStep: 1300 \tLoss: 82.0681 \n",
            "Epoch: 5/5 \tStep: 1301 \tLoss: 81.1217 \n",
            "Epoch: 5/5 \tStep: 1302 \tLoss: 76.4283 \n",
            "Epoch: 5/5 \tStep: 1303 \tLoss: 80.0795 \n",
            "Epoch: 5/5 \tStep: 1304 \tLoss: 79.5099 \n",
            "Epoch: 5/5 \tStep: 1305 \tLoss: 79.9852 \n",
            "Epoch: 5/5 \tStep: 1306 \tLoss: 78.9763 \n",
            "Epoch: 5/5 \tStep: 1307 \tLoss: 90.0806 \n",
            "Epoch: 5/5 \tStep: 1308 \tLoss: 79.8613 \n",
            "Epoch: 5/5 \tStep: 1309 \tLoss: 80.2240 \n",
            "Epoch: 5/5 \tStep: 1310 \tLoss: 76.5649 \n",
            "Epoch: 5/5 \tStep: 1311 \tLoss: 79.5054 \n",
            "Epoch: 5/5 \tStep: 1312 \tLoss: 76.2971 \n",
            "Epoch: 5/5 \tStep: 1313 \tLoss: 81.4919 \n",
            "Epoch: 5/5 \tStep: 1314 \tLoss: 77.6080 \n",
            "Epoch: 5/5 \tStep: 1315 \tLoss: 80.7552 \n",
            "Epoch: 5/5 \tStep: 1316 \tLoss: 90.3823 \n",
            "Epoch: 5/5 \tStep: 1317 \tLoss: 79.5312 \n",
            "Epoch: 5/5 \tStep: 1318 \tLoss: 89.6293 \n",
            "Epoch: 5/5 \tStep: 1319 \tLoss: 80.7372 \n",
            "Epoch: 5/5 \tStep: 1320 \tLoss: 79.8876 \n",
            "Epoch: 5/5 \tStep: 1321 \tLoss: 79.8337 \n",
            "Epoch: 5/5 \tStep: 1322 \tLoss: 77.8233 \n",
            "Epoch: 5/5 \tStep: 1323 \tLoss: 81.0155 \n",
            "Epoch: 5/5 \tStep: 1324 \tLoss: 77.3486 \n",
            "Epoch: 5/5 \tStep: 1325 \tLoss: 77.2380 \n",
            "Epoch: 5/5 \tStep: 1326 \tLoss: 81.1644 \n",
            "Epoch: 5/5 \tStep: 1327 \tLoss: 87.2132 \n",
            "Epoch: 5/5 \tStep: 1328 \tLoss: 80.4730 \n",
            "Epoch: 5/5 \tStep: 1329 \tLoss: 81.7608 \n",
            "Epoch: 5/5 \tStep: 1330 \tLoss: 79.0342 \n",
            "Epoch: 5/5 \tStep: 1331 \tLoss: 80.4853 \n",
            "Epoch: 5/5 \tStep: 1332 \tLoss: 79.7413 \n",
            "Epoch: 5/5 \tStep: 1333 \tLoss: 80.0924 \n",
            "Epoch: 5/5 \tStep: 1334 \tLoss: 80.4046 \n",
            "Epoch: 5/5 \tStep: 1335 \tLoss: 78.3031 \n",
            "Epoch: 5/5 \tStep: 1336 \tLoss: 76.9791 \n",
            "Epoch: 5/5 \tStep: 1337 \tLoss: 77.8338 \n",
            "Epoch: 5/5 \tStep: 1338 \tLoss: 80.4094 \n",
            "Epoch: 5/5 \tStep: 1339 \tLoss: 76.3433 \n",
            "Epoch: 5/5 \tStep: 1340 \tLoss: 81.3113 \n",
            "Epoch: 5/5 \tStep: 1341 \tLoss: 77.7675 \n",
            "Epoch: 5/5 \tStep: 1342 \tLoss: 81.8680 \n",
            "Epoch: 5/5 \tStep: 1343 \tLoss: 87.8945 \n",
            "Epoch: 5/5 \tStep: 1344 \tLoss: 85.0027 \n",
            "Epoch: 5/5 \tStep: 1345 \tLoss: 81.6807 \n",
            "Epoch: 5/5 \tStep: 1346 \tLoss: 81.5175 \n",
            "Epoch: 5/5 \tStep: 1347 \tLoss: 81.1426 \n",
            "Epoch: 5/5 \tStep: 1348 \tLoss: 76.5300 \n",
            "Epoch: 5/5 \tStep: 1349 \tLoss: 76.7579 \n",
            "Epoch: 5/5 \tStep: 1350 \tLoss: 76.4858 \n",
            "Epoch: 5/5 \tStep: 1351 \tLoss: 81.0086 \n",
            "Epoch: 5/5 \tStep: 1352 \tLoss: 81.2759 \n",
            "Epoch: 5/5 \tStep: 1353 \tLoss: 76.8321 \n",
            "Epoch: 5/5 \tStep: 1354 \tLoss: 80.3989 \n",
            "Epoch: 5/5 \tStep: 1355 \tLoss: 80.4618 \n",
            "Epoch: 5/5 \tStep: 1356 \tLoss: 80.7563 \n",
            "Epoch: 5/5 \tStep: 1357 \tLoss: 76.4880 \n",
            "Epoch: 5/5 \tStep: 1358 \tLoss: 81.0583 \n",
            "Epoch: 5/5 \tStep: 1359 \tLoss: 76.2459 \n",
            "Epoch: 5/5 \tStep: 1360 \tLoss: 79.5257 \n",
            "Epoch: 5/5 \tStep: 1361 \tLoss: 76.1406 \n",
            "Epoch: 5/5 \tStep: 1362 \tLoss: 81.2109 \n",
            "Epoch: 5/5 \tStep: 1363 \tLoss: 76.8800 \n",
            "Epoch: 5/5 \tStep: 1364 \tLoss: 81.8019 \n",
            "Epoch: 5/5 \tStep: 1365 \tLoss: 80.6308 \n",
            "Epoch: 5/5 \tStep: 1366 \tLoss: 76.6013 \n",
            "Epoch: 5/5 \tStep: 1367 \tLoss: 80.2702 \n",
            "Epoch: 5/5 \tStep: 1368 \tLoss: 76.4127 \n",
            "Epoch: 5/5 \tStep: 1369 \tLoss: 80.9663 \n",
            "Epoch: 5/5 \tStep: 1370 \tLoss: 80.1742 \n",
            "Epoch: 5/5 \tStep: 1371 \tLoss: 80.5007 \n",
            "Epoch: 5/5 \tStep: 1372 \tLoss: 79.4301 \n",
            "Epoch: 5/5 \tStep: 1373 \tLoss: 80.5006 \n",
            "Epoch: 5/5 \tStep: 1374 \tLoss: 77.4595 \n",
            "Epoch: 5/5 \tStep: 1375 \tLoss: 80.3870 \n",
            "Epoch: 5/5 \tStep: 1376 \tLoss: 89.2279 \n",
            "Epoch: 5/5 \tStep: 1377 \tLoss: 80.6763 \n",
            "Epoch: 5/5 \tStep: 1378 \tLoss: 76.6917 \n",
            "Epoch: 5/5 \tStep: 1379 \tLoss: 80.2939 \n",
            "Epoch: 5/5 \tStep: 1380 \tLoss: 78.4418 \n",
            "Epoch: 5/5 \tStep: 1381 \tLoss: 91.3387 \n",
            "Epoch: 5/5 \tStep: 1382 \tLoss: 76.2739 \n",
            "Epoch: 5/5 \tStep: 1383 \tLoss: 80.5143 \n",
            "Epoch: 5/5 \tStep: 1384 \tLoss: 81.0788 \n",
            "Epoch: 5/5 \tStep: 1385 \tLoss: 76.4976 \n",
            "Epoch: 5/5 \tStep: 1386 \tLoss: 76.3170 \n",
            "Epoch: 5/5 \tStep: 1387 \tLoss: 79.8464 \n",
            "Epoch: 5/5 \tStep: 1388 \tLoss: 79.1888 \n",
            "Epoch: 5/5 \tStep: 1389 \tLoss: 79.9548 \n",
            "Epoch: 5/5 \tStep: 1390 \tLoss: 81.5230 \n",
            "Epoch: 5/5 \tStep: 1391 \tLoss: 86.6146 \n",
            "Epoch: 5/5 \tStep: 1392 \tLoss: 79.6896 \n",
            "Epoch: 5/5 \tStep: 1393 \tLoss: 85.7575 \n",
            "Epoch: 5/5 \tStep: 1394 \tLoss: 77.0876 \n",
            "Epoch: 5/5 \tStep: 1395 \tLoss: 80.9025 \n",
            "Epoch: 5/5 \tStep: 1396 \tLoss: 75.4180 \n",
            "Epoch: 5/5 \tStep: 1397 \tLoss: 85.2436 \n",
            "Epoch: 5/5 \tStep: 1398 \tLoss: 75.3649 \n",
            "Epoch: 5/5 \tStep: 1399 \tLoss: 80.2531 \n",
            "Epoch: 5/5 \tStep: 1400 \tLoss: 80.3608 \n",
            "Epoch: 5/5 \tStep: 1401 \tLoss: 79.1100 \n",
            "Epoch: 5/5 \tStep: 1402 \tLoss: 80.3697 \n",
            "Epoch: 5/5 \tStep: 1403 \tLoss: 81.0870 \n",
            "Epoch: 5/5 \tStep: 1404 \tLoss: 80.8762 \n",
            "Epoch: 5/5 \tStep: 1405 \tLoss: 82.6112 \n",
            "Epoch: 5/5 \tStep: 1406 \tLoss: 81.9827 \n",
            "Epoch: 5/5 \tStep: 1407 \tLoss: 80.6998 \n",
            "Epoch: 5/5 \tStep: 1408 \tLoss: 80.5223 \n",
            "Epoch: 5/5 \tStep: 1409 \tLoss: 80.6930 \n",
            "Epoch: 5/5 \tStep: 1410 \tLoss: 86.8261 \n",
            "Epoch: 5/5 \tStep: 1411 \tLoss: 81.0118 \n",
            "Epoch: 5/5 \tStep: 1412 \tLoss: 87.7870 \n",
            "Epoch: 5/5 \tStep: 1413 \tLoss: 76.2467 \n",
            "Epoch: 5/5 \tStep: 1414 \tLoss: 80.0081 \n",
            "Epoch: 5/5 \tStep: 1415 \tLoss: 78.9065 \n",
            "Epoch: 5/5 \tStep: 1416 \tLoss: 80.5357 \n",
            "Epoch: 5/5 \tStep: 1417 \tLoss: 75.4645 \n",
            "Epoch: 5/5 \tStep: 1418 \tLoss: 77.1899 \n",
            "Epoch: 5/5 \tStep: 1419 \tLoss: 86.8034 \n",
            "Epoch: 5/5 \tStep: 1420 \tLoss: 79.3270 \n",
            "Epoch: 5/5 \tStep: 1421 \tLoss: 76.5343 \n",
            "Epoch: 5/5 \tStep: 1422 \tLoss: 79.5577 \n",
            "Epoch: 5/5 \tStep: 1423 \tLoss: 78.8187 \n",
            "Epoch: 5/5 \tStep: 1424 \tLoss: 81.3777 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgojLXC84U4Q",
        "outputId": "c27179a1-2b36-4c3c-9b62-efc155b98a32"
      },
      "source": [
        "# loading the weights of the pre-trained model\n",
        "model = Seq2Seq(dropout,num_hidden,enc_layers,weight_embedding,\n",
        "                vocab_size,dec_layers,embed_dims,device).to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/LSTM/weight.pth'))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning:\n",
            "\n",
            "dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Beam Search Loss"
      ],
      "metadata": {
        "id": "agg1Z-9apAvG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys0FqjDiEsNi",
        "outputId": "f87c9fd1-51dc-44ab-be27-beb34497e7f2"
      },
      "source": [
        "# beam search decoder\n",
        "with torch.no_grad():\n",
        "    beam_time = time.time() # start timer\n",
        "    loss_beam = [] \n",
        "    beam_predict = [] # save beam search decoder outputs\n",
        "    highlight_validation = [] # save validation target summaries\n",
        "    story_validation = [] # save validation input text\n",
        "    model.eval()\n",
        "    # initialize the encoder hidden state\n",
        "    val_hidden = model.encoder.init_hidden(batch_size)\n",
        "    for x_val, y_val in get_batches(val_story, val_highlight,batch_size):\n",
        "        # convert data to PyTorch tensor\n",
        "        x_val = torch.from_numpy(x_val).to(device)\n",
        "        y_val = torch.from_numpy(y_val).to(device)\n",
        "        val_hidden = tuple([each.data for each in val_hidden])\n",
        "        # run the beam search decoder\n",
        "        val_loss, prediction = model.inference_beam(x_val, y_val,val_hidden,criterion,beam_width=3,batch_size=256)\n",
        "        loss_beam.append(val_loss.item())\n",
        "        beam_predict.append(prediction)\n",
        "        highlight_validation.append(y_val)\n",
        "        story_validation.append(x_val)\n",
        "    model.train()\n",
        "    print(\"Beam Test: {0} s\".format(time.time()-beam_time))\n",
        "    print(\"Val Beam Search Loss: {:.4f}\".format(np.mean(loss_beam)))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Test: 143.65502405166626 s\n",
            "Val Beam Search Loss: 22.6244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U96lynicoMZV"
      },
      "source": [
        "# predicting the output\n",
        "result = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    beam_time = time.time() # start timer\n",
        "    loss_beam = [] \n",
        "    beam_predict = [] # save beam search decoder outputs\n",
        "    highlight_validation = [] # save validation target summaries\n",
        "    story_validation = [] # save validation input text\n",
        "    model.eval()\n",
        "    # initialize the encoder hidden state\n",
        "    val_hidden = model.encoder.init_hidden(batch_size)\n",
        "\n",
        "    for x_val, y_val in get_batches(val_story[:batch_size*7], val_highlight[:batch_size*7],batch_size):\n",
        "        # convert data to PyTorch tensor\n",
        "        x_val = torch.from_numpy(x_val).to(device)\n",
        "        y_val = torch.from_numpy(y_val).to(device)\n",
        "        val_hidden = tuple([each.data for each in val_hidden])\n",
        "        # run the beam search decoder\n",
        "        temp = model.infer(x_val, y_val,val_hidden,batch_size=256)\n",
        "        result.append(temp)\n",
        "    model.train()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK7HDLPh-_CZ"
      },
      "source": [
        "#index to word dictionary\n",
        "idx2word = dict((v,k) for k,v in word2idx.items())"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRqFFU6OvdiC"
      },
      "source": [
        "# concatenating the actual output\n",
        "def concatenate__reference(refer):\n",
        "\n",
        "  op = refer[: batch_size, :]\n",
        "\n",
        "  for i in range(1, 7):\n",
        "    op = np.append(op, refer[batch_size*i : batch_size*(i+1), :], axis=0)\n",
        "  return op\n",
        "\n",
        "reference = concatenate__reference(val_highlight)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXyrB44cOD3n"
      },
      "source": [
        "# concatenating all the predicted result\n",
        "def concatenate_result_model_output(output):\n",
        "\n",
        "  op = output[0]\n",
        "\n",
        "  for i in range(1, 7):\n",
        "    op = np.append(op, output[i], axis=0)\n",
        "  return op\n",
        "\n",
        "model_out = concatenate_result_model_output(result)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj_vUaEvZqQX"
      },
      "source": [
        "# converting index to word \n",
        "def numpy_to_str(array_):\n",
        "\n",
        "  result = []\n",
        "  for i in range(array_.shape[0]):\n",
        "\n",
        "    temp = \"\"\n",
        "    for j in range(array_.shape[1]):\n",
        "      temp += idx2word[array_[i][j]] + ' '\n",
        "    result.append(temp)\n",
        "  return result\n",
        "  \n",
        "reference = numpy_to_str(reference)\n",
        "model_out = numpy_to_str(model_out)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYcCxkEe-Cm1"
      },
      "source": [
        "# calculates rouge score\n",
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(reference, model_out, avg=True)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7zDYPjd9k3M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c12a0779-6c9a-44df-9e88-926681e572a4"
      },
      "source": [
        "scores"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge-1': {'f': 0.4121747902755441, 'p': 0.26212201736755253, 'r': 1.0},\n",
              " 'rouge-2': {'f': 0.1394088826340569,\n",
              "  'p': 0.0924504857122046,\n",
              "  'r': 0.3108258928571428},\n",
              " 'rouge-l': {'f': 0.4121747902755441, 'p': 0.26212201736755253, 'r': 1.0}}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4diB1LOB9k0h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison\n",
        "\n",
        "### **Pointer and Attention mechanism**  \n",
        "#### 1. ROUGE SCORE\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAvsAAACFCAYAAADFNjfWAAAgAElEQVR4nO2dT2vb2Pf/3/Pj8xQMWUxhLEwfgAkYQhcqdGG6Fl5/YtwshqxT4X1dNevShWvMbKPRuvgLA9ViCJg6fgCDEQMtNNQP4LPUb3ElWX+upGtZSRz3/QJD68hXR/fvueeec/TL//73Px+EEEIIIYSQg+MX3/ep7BNCCCGEEHKA/L+HFoAQQgghhBByN1DZJ4QQQggh5EChsk8IIYQQQsiBQmWfEEIIIYSQA4XKPiGEEEIIIQcKlX1CCCGEEEIOFCr7hBBCCCGEHChU9gkhhBBCCDlQqOwTQgghhBByoFDZJ4QQQggh5EChsk8IIYQQQsiBQmWfEEIIIYSQA6V2ZX/t9KFpGjStD/tWcsGtjb6mQevbWNd980PlwepsiZGmifa0lvIrLPH30aKkqPAZok+2f4R9p++wZxByL9Q6t4TzxQjy2YIQ8jDUOzaV1/1HSbne8xj5T62lLUboXADWtYfeUa0lk3tnDbtvYHLmwDPbuxV1a6N/YsKtoyxCCCGEkDuhjaHnYYglRpoBDYeht9Rq2V/+NQHOzosV/aMWWgDwtIVGnTd/lKxh97Vya/ZD1NntZ8xcHdZ/izv5k5YOQEfr1/xr1n/P4KK8rMZvLQBA67f7ecrNKVSNu/jYCYbc6lHe5hm5CqyuoYVFJnv0N+mn5OQtJX9GptQn8azpU5w8+TOnPRKrU+mJUMwKI/tE997Ue/qTbIf88nKtWKVtnq7DHOtaVE5O29wFtc4tTyCmgxae7FyWCum22qHeFE4ec6/N9O9sHyrvO/n3y4xj2TylLL96ndUxt0jnjYJ5Nn39ZmyqjvOCuSrRTurjvHw+3mVuua+xXu/YVFn360VRV1IpSXndb+PVpQ78szoMLxS/Nn74V6dNv/n2puS6G/9NU+W6w+bHn6d+s3nqX3258k+bTf/0zx8FVz9AnX15I+T7XnxZ9BwF14lr3vil0n954zebTf/Nl22F3Z6bt82k3N9FO+xWx8EYaDalz6HS5j/+PPWbp1f+5i9B2ye+82N1tX3fuHkrKU9BfhmZtpW0oajrVPsH123qILx37LrvV/5pqm9l2i3/Kf03svIV56fi8RhHsc7C/tWU1EVYxumVf6Mwnuqlzrll8xyqtVcZyXhV7xuystT6Wfk9RH3G+4EYI+m+odbmN29TbSObp1TlV62zO5lbCuQXv5TPc+V3zIxXtfVGbZyrzceqc4tEiqp9dmvqHZsq635dbKcrFbPtuq+suzwCHkDZVywjsTjKGjgYdNEnZ2JOd+6gXOnE28x+kvdNX1exs395s5Hr++4dOJJOuqhURFHZV6HuAbPzBCntA/7uzxz+/ouk/B3aPDOxfr/yT6P63HJBLrp3kfxS0vfOWUAz98ybKxQWYMW6yy5Gd6TsK9ZZOBfdSMbCzdvN/e5zES1no7iGc4t8s7IlNWys5UplHWtQgKyfVZ4fsmN0lzaXbp4V5Feqs7uaW9Lyp+Qo3STkIKu7OpV9tXvu0O/y1qJ7ZlNnKR1np81B1Q1cjDp1pQrr/iEp+/uZjWdsQDuZoXvtwfM8ePYA7kVnc7y2GEHTDKwu5+Lvnof5JWCeVAkYWcPud2DCwjwqSwcADGwPUyM83F5ipHVgPnVS96xwDHc8hDft1eySs8bnTy4AYPLXHgSVLEbRMVnnwgUwgaF4lFvMErMxALiY/V3tcE24FQ3QPU58C/vDZIdylxj1JtAv36EnO9qss82Peph6Q1TxIlz+YcLFAOdGWpIS+SWsnfeYxN2zbldYQeKGdfQcXR1wV1+D6z5j5gKDF21JeYD76fOOx6ZLfLxwy10Kd0axzhYjGGMd1qgnPUJvm/F5Zv+Y9DR0PnWD+XEOS5/A2CGgV4w/AONZxWBBMQfoL58nx9PiI0x3l3KLCOaHmvrU/be5Yp3dydxSwq2N92Ng8Pu28+N9jfOfhQkMrYPZy1CvcjBwTXSqrtWLGSYA4M7wuaqrUo3r5t2s+4+H/VT2MYDjTTcD+LiLAYDVv2tsJl0nMVk2jHewdGDyYctFKFQ8YhNNwzjHAEmlWSgiAzixQA1xTxfmH3ugXKOB5y+DTcqLPQgmOR7GNkU6RJt60XfVA17a6J4BgI7us2pTwNeVm/JdFBu52UsHVlwp3YKlZWCiW3h3Bwv415ULoIXWrgtasKjql68yi/n28ksW2sD/W4xTCaHv47cV3JS/59ISCqVzqQPuCrkt8G0FF8VxHZlNSJyxUeBTu8G96JT6XKvVWWxD8FgVEt3CPFpwG+j9PthpAW8860IHgLNuJaVStqlcO31oPcCxBwBWWO3qB53pZ18RTBsZ/+zyTGRCXr1Vh7f0Gqt/UO57nZb/ruusYG5JsBjBSCv24XwAhRieGIXjPG1cytmcqozzNLnzseLcEnsC2EMTrm7h1XHJpfdE0sAZrLVVfdYDvQ16F8/3YO6rsu6HcYSHQH3ZeCKluQZFMzORiehocR8bMxfQX6anukDZvRCKgrLKFU6KUjHCewRW87Pz1ETWQOspgsHQfvCA44YxhWfUU9byrwmgW3sxSNO0TQ+eWVNhixG03grWtYfh0Rr2pwplhJYpu+7TGkSLY2XFKEZoeXPSymkF+eULrVgcjIvXsJ9tNutr57WwHupSqTAKT+nMBtbOrOgJMOpNgIx1JnmN2IQ4KeW6gd7UQy/+1WIErdeBtopnWwgzMcSftY9OT8Pqcr5ZCBXrLDIS7LHlvoyMNfjXFnRMsPoGoMr8cNTD1OuVX6dEcDL71IHntYHFqIYyJf0sUJbdi/fixDnq26JvwPYwlPbJQKGDDquicSJRWjCW9MvnBf2ubJzUX2e5cwsQjLNJ9F/9cp6oq/W/KwAuzA/i9KgRk9HQAEd60pA3zmXroJhjOtoqVpbiOJc8S3Y+Vp1bNs9luuH/B3C8O1g3KpENuG2bHrzK5WXreG9QXfd/bUGHiffOq70+fVVhd8t+mAVgCLzz8ia8+sm37G1poTh+lTkRCBWZjeVYWHXSO3dN08TAj5PJilBuodgnwiwLsxfeHbga7RdfQ8tW/BRpa4LF/Mypv++Hi6RuYV5D+lO55a2K/PnH523Tg3PmwjzZ9P/XOIelI5vx5ZuNvmYAtopLg1iwJ9BhXee7GRRb+1IcD+GcodTtIzw13LgXKdbZrY3XFy4GdjW3CFJGaJmb15gar7ifpU9oohNlqevkRrEb2LvMMUFpTl+4RKZOtbeR/07qrMyqHzvl9bw5up86kjVRuLnFzurRG1nQMcFMYm3fapyjjaE9AHLKiu6YGecptpmPc+cWsTGI6sMGjBoyzBB1tlr3j3qYeh7OV51aMgE9JLtb9kMrTaDktnItHPfFtu4OgSIPEx0tNBfrsK7jHSFIW/VUId9qrVar+ye0mi8tDdqH+PH94fCkpQNjEwYszBNWleCoPnNqlI+wtA3gTGtW525t9APrnFNDG+RZ3qrIX7bQZk9elhhdxOo1sAybvew7OcRRazd1she88wFlSlO+tS8PkUJOGAjaub8JTvDGgXuRUp1tNgTTPTmiPxgCd7FJzxBuBxkrcVWXt4J+FrqobVHa0uoEVvj57mviYiQU/UJFs1z++uusxKqfQSjxsxMTs8UQ7ePQVWKrmt16nIs5J3AxPM6TMznOE1dVmI+V5pbjIZyzCYxPn7E2Dm+t3ScqrfvBO4Jado3eBA9EfT77QRDenQeH5t4ndLUpdneIgsPC/4fH7HF/8syOL+6us7+E+WPreKtd+8Vufrl3iTh9qJ6fOPQZzgSELWapE52A6LQmbY0Kg6JT/qEnJlyIwMZK7RG+hAyDnCPs7cvLs+pvL3+FoLjFDJN44F4whrNlyIII49bRYkPCdtY+UfbnT66CT2kQFH7WRVu1zv5PuDWmTwOTweoVT/zC4PcHegu5PNBtC8LxVDlIP/Alzvg6568B4YllvmWurJ8Jg0/Wrzf05U8qCktLnPrqRS4hqsQtyrmKZpn829eZEqq++kX82oIOV7iFxZHE9gBVxvnmXS/FcV7xcR6j0nysOrcU8MDjfHeC9wrcp/wFdbb1ug/Fue6xtFN9iX1qTL1ZlqpJkss7PwXX5rt4+rjot6rp1GrJw55f7u6pN2Mps+qQcW9Tb25Sru6ebzed070g9/0WuedL06kVtXmUcla1vsrT4yml6pPIIJO/WppAyfWZMSwb+5s+vX0a0B1kSyDJ/y9DMYVe2VhQf2/FFv2xMpL0hDXMVxv5d5gT8lJLSsuMpWkueb9EYX0qvkNCfLdDal3JPYvXREX5t6oz37+TuSUnHWO2nLx7V3gvROadHjJyxvnW83EgpWJ66KJ02fc3zu8o5W/Yd+sqV2HuCcde4XsrFNd931fTXe6znXahvgDd++R4CM8GtJ6GTdhPdtfdMKZwVhqMEw0mICwj3jk+akaiLOcsdk2cs5jbzlEPU68lXp+c8tMvszhmiCwFMS460C6ArAuRKkGAsuvuRzaeO0NYqCbj6tl4gMC3HBoMLRk4JrPCCYuAu7s1U6HNxZE4IKy/k2QZkWUv9MuNMd70y0R/DCxvdQT4Rlb9gsDtyK845CwIAkwjG8Npy2WYFhDCYp6sjeQ4CdN25vd9SZ3p6eNcyNvpzIFXt5tWjEydQaQRjuaslDW3YZxjcOGm6uPucKN+Kth6vksRjadd+uRRD9NroH8Sly3P8hrOGZJgY0C9n+X12XgfCscbsvUGbOpOrc3DtIAA3LibaUC4PqnKr1Rn9c4t4QlHug68VP/Jm4+9jNth2TiX3VPPuAuqjvPK87Hq3KJbmHtT6WnNfY/z2jnuYoAJJrucbmypK7VfDIDxJPdEZZt1X5XH0k6/+L7v11NULMK/tmCpu0dMDBIFOzg6reUY9jESRavvHlhGCKmX3Hmr3rtEmZJ+yjmQkAfmfsY5yUNsylul7luPoZ1qzLMf+LU/KsKcxZJdYBDQ89Pysz8/IftKkP7vUefuJ4QUw3H+OHgk7VTrS7VEtPP7yoGT90+wQckEom5yI+/iKvKoOWqhhX15YRghJAp4601SL78hhBwOHOf7QZiMIs9N7XG1U41uPIKNL+J+H2nEkfkVFmc++FmI+SKePS73LELIrtCNhxDysxHTew5ID6xd2SeEEEIIIYTsB7W68RBCCCGEEEL2Byr7hBBCCCGEHChU9gkhhBBCCDlQqOwTQgghhBByoNy/sn9ro69p0Po21vd+80fKg9VZkFpK06BZ8hScS0v8fbQoKSp8hujTz6RoXTt9aJqGvsOeQQghhBBSB/95aAHIvrKG3TcwqSPlZvjKa6bvJIQQQgi5V+7fsn/UQgsAnrYOIndpZRQs3REPUWe3nzFzdVj/LVbOn7R0ADpav+Zfs/57BhflZTV+awEAWr/dz1OGJwlayemFMotRUNYI2ZJipyRlbZ7uGyUnOpvnkN13c/oSfqQnJ5Hsm4/stCZdlvSemb4tv2emrIL6T1+bkE0i+13XWabvlNw3fX2mPrZsc0IIIUQZ/9658d80m37z7c3933pf+H7lnzZP/avvm69u3jb9Zuq72F/vv86+vCmQZ8OPP09LrxPXvPFLpf/yxm82m/6bL9sKuz2Z+v5+5Z9WruNN+4hyFZ5VJoPvS+ugsMxQ7qbsmh/+1Wnq+6D80z9/bK6StKH4Li5HUNbplR/7ZW75iTaU3PPmbaqu8+q/UrsE7ZGQVVLmznWm0s4lssTKV25zQgghZAseQNkvIlho396kFuTkQisIFtHok1U4b95KFtmg3KRCGS7w2U/yvunrypVhZQK5ss+pTlZB2wFFZV8FdcVIjeKNkQLSPuBXfGbRJ8KytlLSMnKIPp3pAwV9I+zjN7I6znmeZHvExlyy5KQsinUmHXO595A8S+IZ1H4no6jP7V5n6n1aXh+JK7Zuc0IIIWQb9jMbz9iAdjJD99qD53nw7AHci87m6H4xgha8xt3zxDXzS8A8UQgUzbCG3e/AhIV5VJYOABjYXuw18UuMtA7Mp07qngXuN/fKGp8/uQCAyV87uqPUQcy1onPhApjAqMVlZonZGABczP6u5ugg3IoG6B4nvoX9YVKh3AZ6Uw/D4/IrS7ldYQWJG9PRc3R1wF19TX6/GMEY67BGPTyRFLf8awLoXTw/SnyLj0F7zOJj5Z9V0m0kR5bVvyl3ln9XAFpoxe/hrpCU9CtWLqC3ZFIWsPgIU8GVbLsya6yzMm5tvB8Dg98LXre+bZsTQgghW7Kfyj4GcLwpeuGCe9zFAKGiEShlZ05MEQcaxjtYOjD5sKWv6+1nzNzkgtwwzjFAUmleO+8xwQBOLMBU3NOF+UcNyvW3FVzs4q/ewPOXwSblxR4EwR4PY5siHaJNvei76oG6bXTPAEBH91m1uvoqNM+Ysic2crOXDqx7U7DWsIcmXN3Cq3CjEMRmpBXqiIRCvsSoN4F++W4zTlLlr/5BMs7j1kZfMwDbiY2nBnojC7prohP6r8cCqqNNzFEP7y51uBedaKO2dvroXLgY2EOErdk2HQwwgRHFIywx0gxMdAvvjKL2CuSNtUu0kfg22sofH7c2Xl+40C9fIdnL6qqzkNQGNi3XtxVc6GghHcMQixPYqs0JIYSQ7dnPbDwJRQwA2hh6nvjnrY2ZC+gv03a5QNm9EFZFZTUwVLKlYkRqh7Can52nlIcGWk8RLMjtHYJnhRKCjLV5OxrGFJ5R/fcJif6aALqVsnDuB23Tg2fWVNhiBK23gnXtYXi0hv2ppnKlBKdIbvj/ARwvbvUVGxnj4jXsZ5vN7tp5LX6jx0oKN5+FCnT8+j46Fy04noc2lhjF/3jUw9R7DrvfgaFNAAD65RxequyGMYX3zEb/xIA2BgAd1rWXUpzFWO1aGowTDSYAKGRhCp9Rv3we1cfXlQvAhfGXA88bBt+KzUOnD8ynvaRSfmIiqtozJyN/nXWWHWuBXNoKjic2P2Kz4sL80MXc8wJZ10E9I7hOvc0JIYSQKuypZb+cfAv4Cqtt3GqOX2VOBIRSELccCzcEjI1M9g1jnCpPkokkL+OHILB8Qod1PcRD2+TDLCSzFx68aYH7wQHw1elD6yF5inSnCJef6HTDBoxUZpa26cE5c2GebPrPa5zD0rGxOAeW67hFvYilpaHzqYu5l3P9rY2+FnNlC9zm0v127fShnZhA4D4XyZlwyRJZh4xxcJJzbUEfG4XZpsITgvRpnSB5mga0MbQHgDvD53h5Rz1MYydHDlL3rLvOMgRyZVx9hMtQ7AxSnKTErlNqc0IIIaQij1bZzyflP1xKoMi7JjqRjzlgXccVwCdo6RDWwrgrSviJK8UppUN88hSGIJc9gIF9XwpnMW1TyNz963DT/z1p6YBrwsgocxV9y6tyPIRzBrifPifqOWyD8DM1EJMrcP+Ju9hICU6dxgYMOMk+mvATX2J0ItyJIkv58VAo6ZjACBX5xQidCxf65TxSyNtm4KI1NoJYmbA/DyLrthgPDgZwYQ4l/SkoF7qFecr6L9K6VkO4E4XxF3XXWQ6/tqBj45ITppJVk7eozQkhhJDqPD5lPwhcywahhq423UJLnAjOjP0/PNpPKOdpxTvurlMXG5eOgV1PgGeYy3v7IOUs7RcS6+meIE4fqgdGN551oUMSOLmYpU50AqLTmqITmjtkMcMEA5wbjSjGJH3KlAyCFnK2XwwAyfsNEgHKgRKbsSCH73YI+rxwSckqu6FCKxTcYOOcccMLNsvpwN3FCFpPuIvNJadIouzsSZ00KLiIuussh/B9ElH/+bUFHS5W31IXhr78Be+mSLR55m8j5uInhBCizkOnA0oiy+UtQZKXOj9X+Oa7MDVl4req6RZ3ysOeZpPCs76c8rG0oHXIuLepNzcpV3dJS5hJ81iQ6lDab1TLLUAlVapqmlF5HUvGU2bshP1Gnmc/nXpTnmc/lXozJ89+ol+G3xWOdYn8SikpJbnyZVdVrjMJklz8vi/rD+XvzChr8236IyGEELKfAbplHA/h2YDW0zCJvoy5DgQ0jCmcVSxQULcw987xUTMSZTlnsWvixAMLj3qYei2MtDA4ccPWlvnFxyhIc5J4BkAEPVZx6QkClF13P7Lx3BkioHEyrp6NBwj8pKFFAakAEi4qccRJgJtr2V1a6diNyabcsA+lA0iBoD9OExbtyH895MyB51VtzwZ60znQ76CjbXp3sr9urjFTYyBx3VEP02ugf2ImykqPu6heU/06WbdhmlME7nOpkReNOxX5JXUW3M+bVukfavfMtrksWDm/n8WDh7dt84ZxjsGFm5o3CCGEEDm/+L7vP7QQD4lYtCUKduBikKcAHjxRlpr9iCUghGzInbcIIYSQFI/PZ79Wwtze6ZfoIAq2+2n52Z+fkH1lMYIxRsH7AgghhJANP7myHwTeZgJRg+wdO7y46dFz1EILNb0wjBBSAyKtqdabpN7uTQghhOTz07vxADL/W+RmCPm5CN8BAKUXIxFCCCGEkP2Cyj4hhBBCCCEHyk/uxkMIIYQQQsjhQmWfEEIIIYSQA4XKPiGEEEIIIQcKlX1CCCGEEEIOlL1V9tdOH5qmoe+sdy/s1kZf06D1bdRQGiGEEEIIIY+CvVX2CSGEEEIIIbuxt8p+47cWAKD1Ww2Z7o9aaAHA09a95M0PTyWij7XDi6nCU4noM4KstKWlJe45WijIVXbSEbt3srzg5T6ST/a+6Wv7sG/T18hklD3nGnZfK3yGzDNKrsu9Ju8ZonrIlz1d/9I2z7RlXnl11plcPln/IIQQQsgB4u8rX974zWbTf/OljsJu/DfNpt98e1NHYcV3etv0m81T/+p78MX3K/+06r2DOjj980fwxQ//6rTpN5tv/E1pwXenV354VXjPze98/8efp8lrwjpJfBcnvFdT0g7it/HypUiePVM/suub6WeMyZuox+wz/PjzVPJbNbK/3dTtzZ+nuXLfvE3JJWvz71f+aer30rqotc5yZCGEEELIT8P+KvsFbBTXQNkLPqXKZwmFSpUKgWKV2aB8eVOh3EDRzChpSUU7T7kVz1Ks9P4oUGAjmb/InklN2b95K9tM5D3X5vob2TPl1GH6+asr+9nNxM3bdD2rt6FK/cs2ZbXWWcHvCCGEEPJzsLduPKW4JjqaAdgePM+DZw/gXnR2cE9YYjYGABezv6uF8a7/nsHFAN3jxLewP0y2L/f2M2YuMHjRTt7DeY8JAPfTZ6wBfF25gN7Ck9TP2y8GAFZYFbh/5LPEqDeBfvkOvV+r/F6UMRsD+svnSdepxUeYLoDxLOlyshjBGOuwRr3Ms2xwsfqW/Cbv+bdF1KsO67+b+m6bHqbGfTh+hdRcZ4uPMN3kMxFCCCHk5+LxKvsABraHYahYH7+CpQOTv6r6x7fRPQMAHd1n1RS8rOK5xEjrYPbSgaUD7uqremHfVnChoxVTtpeWhs6nLpxLHXBX+ArgSWvz7yxZ5TgjL1poHSW/X1oGJrqFdyWKrnvRyfcDv11hhWTMxdrpQ+sBjp3eiMQ2FylZIo6HcM6ASW+ToWlpaZGym5R0AmOb2AQs8fHCBc7O8++/FWus/kH5JuTbCi5idVRzna3/XQFoofVttF2sBiGEEEIOhkes7Kct6A20ngL4Z1VZkWmbHjxvWo/CtxhB096jde1hauxqdxYBm+9bc3jTpBW38awLHRMYiYBQoQiWyWeMAZx1kbD73tp4PwYGv6cV6DhtDL3gRCX4zC/1hCKeRATWdlbn8Lwh0nZmYVUf4Lxkc9E2Nyc4mqbBGA/gpNqrYUwTcnmeg4FrolMQvCqz6u/C2nkN05VY6BOEbZTux1EpO9eZ2MxNYPzVzdYHFX5CCCHkp+ARK/v7y9fQGlvHxuGbjX7griR1KTnqYXptQR8bMevtDF17AKROBiIWI2i9CaBbmJtxNXINe2jCPXM2JyaKNIx34vQicC/aEJ5uzOGZEmX61sbrCxcDO6vQpllaGrTeBAPbg+fNYenCgl/8LoY2hvYAwAQzqYtXvVb9tdNH58IFzpwCF6AlRpohNhjXsueur86AAZxEGUF9uDN8ruTiRQghhJDHBJX9GhEuNSaMT13ME9bYrxAePltY+H9tQYcLszdD99pLKN8Zd6GjHqYJa/YQT0IXjrQCe2ujH1iUnWnSei8s0mnlUJXgZCV0KQrSnU562Y1K5F5ypL65WDt9GOO461YDvakH5wxwL14XpqYUdQms/s1uCmq16i9GQtHPbKISd4TdNzABMLBTm8Ga6+xJS9/teQghhBDy6DkgZT8nuHGbEqzynOZFCJcaiQvMYoaJNBZgkzc+E1h89BxdHRKLs8pzrvH5kwv98lXGRad/YsLFAE7GNUT8JuPvfmLChfCVL87PHgQ4R25BQQyEbuFVKmD58ydXXBcEISNxKqEJhTmSQ7jfCJeU7EmFUGiLYxNE4LSs/mu06sdPS6Z5LlBr2P0OTDcVbxJRb52Jd1Vkg7TXeRvBxYg+/YQQQsih8dDpgKogS6+olOqwkE0az11SeGbkkKRX9FN/a+blQc+8a0CSUz+vzHR50b22rKO8dKIJZPn//fzUkkopQWWpN3Py7BeVl3lXQfo+auk0y9OUlrRL7L0FhXVZZ50pvoMhWYaCjIQQQgh5NPznoTcb1ZnA0GJBqLqFuVcUVFqGsKpOxtWz8QAiiNSBlpBNv5zn+NsL670rSbEJADgewrMBrachKi1jOQ79v6MLYF17mKYz7PwhLPSZepOWWUJ0QhDjzIE3TT3DUQ/Ta6B/0oF2EX4pO1VQIKoLA9o4X3aRoSf+Q1EfWct9YNXXLTzPy2YT+t/HME80mIn7hqlVEaSDNZOFnDnC7z5MnwlxSpJsAR3WdeDSU2edoYHedA70Owm55KcKQMM4x+DCRUloNyGEEEIeEb/4vu8/tBDbIpSwVkUFiBCSR5TO9LqmrFSEEEIIeVAOyGefELITQTrWwvcdEEIIIeRR8YjdeAgh9bBxBctz8SGEEGVtDQkAABbZSURBVELI44TKPiE/PeIlacOHFoMQQgghtfMoffYJIYQQQggh5dBnnxBCCCGEkAOFyj4hhBBCCCEHCpV9QgghhBBCDpS9VfbXTh+apqHvrHcv7NZGX9Og9W3UUBohh0U4Puoab4QQQgjZG/ZW2SeE3BNHPUw9D961BVx0qPATQgghB8TeKvuN31oAgNZvjd0LO2qhBQBPW6ihtFLCU4noYy3rKaekrKVVds0So0R5fdi3kssWo8x9R4vUNTFrcG5ZknLEZ4S0hJHsBdcAa9j9cvkzZeXUR+k9VeTP1EPZvTfPUK5Ub9ore226LSVtJK2zghOuox7OzwB39bVELkIIIYQ8Gvx95csbv9ls+m++1FHYjf+m2fSbb2/qKKz4Tm+bfrN56l99D774fuWf1nXvvLKiuip4TslvM7L6vv/jz9Oc72Jt8f3KP01dIytLyJX6LsMP/+q06TdPr/wf6e+ab/yYtNlnk/SRm7epa6R1pnhPJflzCO4bly2q2y/ib6d//sj/ffgsTfFJXivqIlt2erzI+kPwXeLZU/fM+RshhBBCHh97a9nH8RCel/M2z8UosuomrbMyazAQvjTIM9uFtxRl5Vi7Vbi18X4MDOwpekfBd0c9TO0BMH5fvdyQwPKKf1Yby+ytjX4PcPLqKmD5hwlXtzCP1UHbnMPSXZh/hLW2xudPLnB2vpEfQMM4xwDA6t/15pm8aeKa9n8t6HAx+3tLF5Dbz5i5wOD3XuzUpYHe7wMAK6yCOltaBiYp+XH8CpYOTD5sLNVtM9XOYZ2NZzFrvNo9d2H5hwkXA3TDNlmM0PnUxdyboverQgFRX3IwyPwxeAlWrL3DNpr8FRsBixkm0GH9t5347atLHXBXoP2eEEIIOXz2V9kvxYV5osGAA8/z4HlzWPoERuUg3CVmY1Hu1gprwPrvWVLBE9/C/jDZqdxCjnqYekOUbGMwGwP6y+dJN6bFR5gukoowkNxMAMDtCivU5FKVQ7SRCFj/uwLQQusIANZY/QOJG1YDz19WV1yL77kDgaKuX77atMvxEN60p+hGtoY9NOGeOYUbODVcrL4lv/m6cgG9hSe7Fk0IIYSQvecRK/uAfjmPWXF3U/yANrpnAKCj+6yaUptVopYYaR3MXjqw9Bp8oRcjGOO0RVoBibK+dvrQeoBjx63ZDfRGFnTXRCc8Jbm10T9RUDy/reBCtiEQm7Lc05ejHt5d6nAvOpF/+9rpo3PhYmCHm5gGWk+R3YREFFnjg41CvF2U7qkov4TQqn9uVOtHa+c1THcAp+QkKkHQxnorpsIfD+GcAZPexud/aWkwxjqskbwPPWnplWQmhBBCyH7yn4cWYBfSimXDmMIzqpcnXEB2FCpkMYLWW8G69jA8WsP+tEs5k+i/+uV8R2vvGna/A/OpA89rA4tR8s9HPUy957D7HRjaJLqnV6i4LjHqTYD0qcbxEJ43zNzb0FawrjduQA1jCu+Zjf6JAW0MADqsay/pJvRiAIxNvHaeYxrKcmvj9YULIF9BFYozoF8mTzVU7qkqfwKZVX8rlvh44UK/fLfF74OTAOiwUhvVtunBezGC1utAuwCAAZyCk6DGby3ANfFx0avhVIEQQgghD82jtuzvK19Dq7mXoxBuQxC7ELoqdT91lKzLcsKThnl+/MKtjb7WgQkLc8+DZw+EBTz3nkuMNEP4hl+XuRMFJwcpl6a104d2YgKXc3ieB+cssKbHM9kcD2OyBFb2IXB+qSPP9Sa01uPM2WwQtrmnovyJ2tjRqh/GJrxT/n2weXNTsSJReRq03gQDO+bqVpQJ6HgIz5uj9aEoww8hhBBCHg0PHSFciVoz9dRHmBElm81EZEApy76ihCTLS/o+2Ww8wfeS3wmZwww0OZlacjMKhRlstmmLlIxBW6brRp5dRlJaXvaYoNyiv1W7Z3nGo9J2zrtOmv2nuO+EGXtkf897HmnmpEIZCCGEEPJYoWU/xq7ZeBrPutAh8akPsqJkYwE2OdDv1oIaxCPoFl6lgodF9p2usMgHft+ZQNjwPQUJn/m4Rbk4E1CCxQwTAIMX4gxABMVKXLKC9yykg2iTiMDjbH0Hrk+6hbkkKHane6bkT0izq1X/LxHInYwRMDABohONuEVe+N8LN6v0yQUQxJBARyuV/Uf45WcDdyMZ9C6eF5xIhRmw+PItQgghZP+hsh+xezaeMM3jpJd66VJvAv3yXdalJ0gBCaRSJhbIODoRKTRfbelP3f6vCLx9nVAWO8lA0KPn6OrIpAldO+8xQTybT0VFP6gLxIJ9ww1SPH3mJoNRQbD0YgRNM7BKxzCUKPo73VMif/xvu/nqB/71XvojUm/qgbtRqNSXKfpAEOeAeGpVIIwJyMRXKBMEPANwLz5WdCcjhBBCyH3xqAN060VYvyfj6tl4AKGwOdCi4FagQCELlGvXzbEUBwpdnIHtwUsoaaHPfIxxGHQaU8aPepheA/2TMFATyAZrNtCbzoF+B+aJhnisckKpD1N2QmR6SdwbehS8GvnMp8qZxuWP5DLR0RJ3TMoWZgVK/N1LKdahwg7ATZcH4MwRsQqK91SSP0BY9bMBshEZ+QFchG2h5wf85pT1Pmhf9yLenhsZh8cI4hwArbfpDwAKN0LliPcRmO6k/FJCCCGEPDi/+L7vP7QQhJD9YGlpMP4p2wwEG8ydNg2EEEIIuQ/oxkMI2YqlFWRfysnVTwghhJD9gW48hBBBFHfwXK7ER+98KM7VTwghhJD9gW48hPzsxOMJwrgGQgghhBwEVPYJIYQQQgg5UOizTwghhBBCyIFCZZ8QQgghhJADhco+IYQQQgghBwqVfUIIIYQQQg6U+1f2b230NQ1a38b63m++L6xh9zVoWvjpw74tuPzB6myJUSijtZRfYYm/jxYlRYXPUPDMa6cPTdPQd37enkHIvVLr3BLOFyPIZwtCyMNQ79hUXvcfJeV6D4CETvMYdBbm2X8QGuhNPfSAIHf56qEFkrCG3TcwqSMVY5jakWkdCSGEELK3tDH0PAyDN8VryNFbjnqYer1Av+mgjzmmxv6+ZvL+LftHLbQA4GmLb99U5SHq7PYzZq4O67/FyvmTlg5AR+vX/GvWf8/gorysxm8tAEDrt/t5yvAkQVPZxasS2+3LrR6bU508a0BGrgKra2hhkcke/U36yTlNypE/I1Pqk3jW9ClOnvyZ0x6J1an0RChmhZF9onunT9M2n2Q75JeXa8UqbfN0HeZY16JySk766qTWueUJxHTQwpOdy1Ih3VY71JvCyWPutZn+ne1D5X0n/36ZcSybp5TlV6+zOuYW6bxRMM+mr9+MTdVxXjBXJdpJfZyXz8e7zC33NdbrHZsq6369lK+byiUpr/ttvLrUgX9WxaeeRz2cnwHu6utOct05/r1z479pNv3m25v7v/U+8uWN32ye+lffiy56gDpTksv3f/x5WnqduOaNXyr9lzd+s9n033zZVtjtuXnbTMr9/co/3bmOf/hXp02/2WxKnyOqqy/iXqd//siW8Oep3zy98jd/Cdo+8Z0fq6vt+8bNW0l5CvLLyLStpA1FXafaP7huUwfhvWPXfb/yT1N9K9Nu+U/pv5GVX1pP6d+VoVhnYf9qSuoiLOP0yr9RGE/1UufcsnkO1dqrjGS8qvcNWVlq/az8HqI+4/1AjJF031Br85u3qbaRzVOq8qvW2Z3MLQXyi1/K57nyO2bGq9p6ozbO1eZj1blFIkXVPrs19Y5NlXW/LlTWTVW2XfdVdZfSfr8HPICyX0Rs0CQWR1kDB4Mu+uRMzOkGCMqVTrzN7Cd53/R1NXR2RaVaBfmiUpHa5VJQ9hXZeYKU9gF/92cOf/9FUv6XN5u++H27SSszsX6/8k+j+txyQS66d5H8UtL3zllAM/fMWxwVFmDFussuRnek7CvWWTgX3UjGws3bzf3ucxEtZ6O4hnOLfLOyJTVsrOWLa3WlK4Osn1WeH7JjdJc2l26eFeRXqrO7mlvS8qfkqKosyequTmVf7Z479Lu8teie2dRZSsfZSYGtuoGLscO6maHCun9Iyv5+ZuMZG9BOZuhee/A8D549gHvR2RyvLUbQNAOry7n4u+dhfgmYJ1UCRtaw+x2YsDCPytIBAAPbi/lgLTHSOjCfOql73uOReyFrfP7kAgAmf+1BeNxiFB2TdS5cABMYike5xSwxGwOAi9nf1Y7zhFvRAN3jxLewP0x2KHeJUW8C/fIderKjzeMhvGmvHjesox6m3hBVoh+Wf5hwMcB5xrewRH4Ja+c9JnH3rNsVVpC4YR09R1ePHXPefsbMBQYv2pLyAPfT5x2DRZf4eOECZ+foHe1UUOl9lOpsMYIx1mGNetIj9Lbp7bWv56SnofOpG8yPc1j6BMYOAb1i/AEYzyoGC4o5QH/5PDmeFh9huruUW0QwP9TUp+6/zRXr7E7mlhJubbwfA4Pft50f72uc/yxMYGgdzF6GepWDgWuiU3WtXswwAQB3hs9VdaQa1827WfcfD/up7GMAx5tuBvBxFwMAq3/X2Ey6TmKybBjvYOnA5MOWi1CoeMQmmoZxjgGSSrNQRAZwYoEa4p4uzD/2QLlGA89fBpuUF3sQBHs8jG2KdIg29aLvqgfqttE9AwAd3WfVpoCvKzfluyg2crOXDiy9mu/d0jIw0S28u4MF/OvKBdBCa9cFLVhU9ctXmcV8e/klC23g/y3GqYTQ9/HbCm7K33NpCYXSudQBd4XcFvi2goviuI7MJiTO2Cjwqd3gXnRKfa7V6iy2IXisColuYR4tuA30fh/stIA3nnWhA8BZt5JSKdtUrp0+tB7g2AMAK6x2NcBk+tlXBNNGxj+7PBOZkFdv1eEtvcbqH5T7Xqflv+s6K5hbEixGMNKKfTgfQCGGJ0bhOE8bl3I2pyrjPE3ufKw4t8SeAPbQhKtbeHVccuk9kTRwBmttmc96HoHeBr2L53sw91VZ98M4wjJEDMN+s5/ZeDITmYiOBgDc2pi5gP4yPdUFyu6FUBSUVa5wUpSKEd4jsJqfnacmsgZaTxEMhvaDBxw3jCk8o56yln9NAN3ai0Gapm168MyaCguyIVnXHoZHa9ifKpQRWqbsmiz3KfmMMaorRjFCy5uTVk4ryC9faMXiYFy8hv1ss1lfO6+F9VA6H4qMB6vLOTyzgbUzK3oCjHoTIGOdSV4jNiFOSrmOZcAKWYyg9TrQVvFsC2Emhviz9tHpaVhdxrItKNZZZCTYY8t9GRlr8K8t6Jhg9Q1AlfkhzGJRC8HJ7FMHntcGFqMaypT0s0BZdi/eixPnqG+LvgHbw1DaJwOFDjqsisaJRGnBWNIvnxf0u7JxUn+d5c4tQDDOJtF/9ct5oq7W/64AuDA/iNOjRkxGQwMc6UlD3jiXrYNijuloq1hZiuNc8izZ+Vh1btk8l+mG/x/A8e5g3ahENuC2bXrwKpeXreO9QXXd/7UFHSbeO68KT+Iav7UA18THRS9nHnh49tSyX06+ZW9LC8Xxq8yJQKjIbCzHwqqT3rlrmiYGfpxMVoRyC8U+EWZZmL3w6nM72VO+hpat+CnS1gSL+ZlT/yAPF0ndwryG9Kdyy1sV+fOPz9umB+fMhXmy6f+vcQ5LRzbjyzcbfc0AbBWXBrFgT6DDus53Myi29qU4HsI5Q6nbR3hquHEvUqyzWxuvL1wM7GpuEaSM0DI3rzGlb3E/S5/QRCfKUtfJjWI3sHeZY4LSnL5wiUydam8j/53UWZlVP3bK63lzdD91JGuicHOLndWjN7KgY4KZxNq+1ThHG0N7AOSUFd0xM85TbDMf584tYmMQ1YcNGDVkmCHqbLXuH/Uw9TycrzrFpzXHQ3jeHK0P6idE981+WvZ3Ylt3h0CRh4mOFpqLdVjX8Y4QpK16qpAnvlar1f0TWs2XlgbtQ/z4/nB40tKBsQkDFuYJq0pwVJ85NcpHWNoGcKY1q3O3NvqBdc6poQ3yLG9V5C9baLMnL0uMLmL1GliGzR5gXXuJCVcctXZTJ3vBOx9QpjTlW/vyEMevwkDQzv1NcII3DtyLlOpssyGY7qml59ESuItNeoZwO8hYiau6vBX0s9BFbYvSllYnsMLPdzcELEZC0S9UNMvlr7/OSqz6GYQSPzsxMVsM0T4OXSW2qtmtx7mYcwIXw+M8OZPjPHFVhflYaW45HsI5m8D49Blr4/DW2n2i0rofvCOoZZd4E8ROCrw99IYAHqNlPwj2y1pSQlebYneHKDgs/H94zB73J8/s+OLuOvtLmD+2jl1l+8Vufrl3iTh9qB4YHfoMZwLCFrPUiU5AdFqTtkaFQdEp/9ATEy5EYGOl9ghfQoZBzhH29uXlWfW3l79CUNxihkk8cC8Yw9kyZEGEcetonqtEcOVW1j5R9udProJPaRAUftZFW7XO/k/EAqVPA5PB6hVP/MLg9wd6C7k80G0LwvFUOUg/8CXO+DrnrwHhiWW+BbWsnwmDT9avN/TlTyoKS0uc+upFLiGqxC3KuYpmmfzb15kSqr76Rfzagg5XuIXFkcT2AFXG+eZdL8VxXvFxHqPSfKw6txTwwON8d4L3Ctyn/AV1tvW6D/W5Trg970dsQi4PnQ4oiWIuWEku7/wUXJvv4unjot+qplOrJQ+7hNpSXMZSZtUh496m3tykXN093246p3tB7vstcs+XplMrSiEWpZxVra/y9HhKqfokMsjkr5YmUHJ9ZgzLxv6mT2+fBnQH2RJI8v/LUEyhVzYW1N9bsUV/rIwkPeGu6e/8uPw7zAl5qSWlZcbSNJe8X6KwPhXfISG+2yG1ruSexWuiovxb1Znv38nckpOOMVtO3r0rvBci804PGTnjfOv5OJBSMT10Ubrs+xvnd5TyN+y7dZWrMPeEY6/wvRWK677vH1bqzcfpxnM8hGcDWk/DJuwnu+tuGFM4Kw3GiQYTEJYR7xwfNSNRlnMWuybOWcxt56iHqdcSr09O+emXWRwzRJaCDWZcxkpuG0GAsuvuRzaeO0NYqCbj6tl4gMC3HBoMLRk4JrPCCYuAu7s1M9XmuOhAuwDibmPiSBwQ1t9Jsoyob4R+uTHGm36Z6I+B5a2OAN/Iql8QuB35FYecBUGAaWRjON33w7SAEBbzZG0kXe3CtJ35fV9SZ3r6OBfydjpz4NXtphUjU2cong8axjkGF26qPu4ON+qngq3nuxTReNqlTx71ML0G+idx2fIsr+GcIQk2BtT7WV6fjfehcLwhW2/Apu7U2jxMCwjAjbuZBoTrk6r8SnVW79wSnnCk68BL9Z+8+djLuB2WjXPZPfWMu6DqOK88H6vOLbqFuTeVrvf3Pc5r57iLASaY7GLxVlw3Q9ovBsA438q+zbp/aPzi+77/0EI8JGJiyHaa8Oj0Z+kIGSIftN0Dywgh9ZI7b9V7lyhT0k85BxLywNzPOCd5iE15q9R9a2lpMP7Z7xjHx+ezXythzmLJLjAI6Plp+dmfn5B9JUj/96hz9xNCiuE4JzXykyv7QeBtJhB1kxt5F1eRR81RCy3sywvDCCFRwFtvknr5DSHkcOA43w/CZBQl7oZhkLrMRXCP+OndeAC5X2F13/lDIuaLeKaQdpQQckDQjYcQ8rMR03uK9MB4PMEj0I+o7BNCCCGEEHKg/ORuPIQQQgghhBwuVPYJIYQQQgg5UKjsE0IIIYQQcqBQ2SeEEEIIIeRAobJPCCGEEELIgUJlnxBCCCGEkAOFyj4hhBBCCCEHCpV9QgghhBBCDhQq+4QQQgghhBwoVPYJIYQQQgg5UKjsE0IIIYQQcqBQ2SeEEEIIIeRAobJPCCGEEELIgUJlnxBCCCGEkAPl/wNrMz8gBl/wKgAAAABJRU5ErkJggg==)\n",
        "\n",
        "#### 2. CrossEntropyLoss (change in loss over epoch)\n",
        "##### Epoch-1 (123)  to  Epoch-5 (80)\n",
        "<br>\n",
        "\n",
        "\n",
        "### **Vanilla LSTM**\n",
        "#### 1. ROUGE SCORE\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "#### 2. CrossEntropyLoss (change in loss over epoch)\n",
        "##### Epoch-1 (11)  to  Epoch-5 (7.05)"
      ],
      "metadata": {
        "id": "LhpuWy69rmxh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBu36gae9juf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}